{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SE4_multi_choice.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "O3CSRuKkGt2E",
        "hmN18vXMGt4N",
        "CjO9g_QsGt6d",
        "rb9nh89wGt9P",
        "rQIH9OPrGwPq",
        "o_MKLf-2GweR",
        "rlQ3dP6JWUMy",
        "sj66rRTTXPB0",
        "ZNg5M2QNXXbU",
        "luE3KtO7Xb08"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54ed402b73664159b46a865b33e8a780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1835eb540e7749beb212253a34f16d61",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_443bc3f343314b5d873563df4245ac09",
              "IPY_MODEL_89e89d4486544a5d85697294eaa2f5ae"
            ]
          }
        },
        "1835eb540e7749beb212253a34f16d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "443bc3f343314b5d873563df4245ac09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_55c02caa47ec435a8ecbec851c67ce62",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad5f7b0914814b5395cbf34fcf5c5ea8"
          }
        },
        "89e89d4486544a5d85697294eaa2f5ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1f5fb87ebcc4d798a76c4c668755488",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 986kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b65c0d3d530d42f9a87c0e451028f856"
          }
        },
        "55c02caa47ec435a8ecbec851c67ce62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad5f7b0914814b5395cbf34fcf5c5ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1f5fb87ebcc4d798a76c4c668755488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b65c0d3d530d42f9a87c0e451028f856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66bb522fa1d64b94abf09a8d117959ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34ecb434d71c447cbf4d691443a72fec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aab834d947584b35b76f1932ddcc7b79",
              "IPY_MODEL_c7f3fed236734147b32bf9d21e50d407"
            ]
          }
        },
        "34ecb434d71c447cbf4d691443a72fec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aab834d947584b35b76f1932ddcc7b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8f2fe6f8811d4361a80842b47020072d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d597b7c7d94746c7b052fc33cd11731b"
          }
        },
        "c7f3fed236734147b32bf9d21e50d407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_07718df366b3493c847903cd9534b929",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.91kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc3170c6945143d7aa7305ffcf5d2a69"
          }
        },
        "8f2fe6f8811d4361a80842b47020072d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d597b7c7d94746c7b052fc33cd11731b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07718df366b3493c847903cd9534b929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc3170c6945143d7aa7305ffcf5d2a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "915bff56c757455ba81723d09c3ecee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1058b8e4ab00420d8921bd8d62229e02",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7000da1013274b32b0d69c218f1532dd",
              "IPY_MODEL_f963c35fc09c46caa3e6d79841d10e88"
            ]
          }
        },
        "1058b8e4ab00420d8921bd8d62229e02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7000da1013274b32b0d69c218f1532dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_28b61ef24ffc4560a94d560eea99a636",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65bc2805423e41caadbb532de85f783d"
          }
        },
        "f963c35fc09c46caa3e6d79841d10e88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71e944c28ac84f918f42ed77be82924d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:06&lt;00:00, 70.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92252f2e128542b9a4868def2fdf70a9"
          }
        },
        "28b61ef24ffc4560a94d560eea99a636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65bc2805423e41caadbb532de85f783d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71e944c28ac84f918f42ed77be82924d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92252f2e128542b9a4868def2fdf70a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoWyMHHSGxLk",
        "colab_type": "text"
      },
      "source": [
        "# Multi Choice Model\n",
        "\n",
        "\n",
        "*   Take the train dataset generated from the 4 way self ensemble training and train a multi choice model.\n",
        "*   Use Huggingface transformers: BertForMultipleChoice\n",
        "*   Will preprocess and batch the text\n",
        "*   Evaluate results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3CSRuKkGt2E",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: File set up. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su1DUrXVGu0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1906ef72-0810-446d-f959-c8faede4b78c"
      },
      "source": [
        "#Mount my drive so that I can access the split training sets. \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpKvepHKHnBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#copy the training data to colab\n",
        "\n",
        "%cp -R /content/drive/My\\ Drive/train_with_split.csv /content/\n",
        "%cp -R /content/drive/My\\ Drive/dev_with_split.csv /content/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmN18vXMGt4N",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Set up GPU and HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdxA13PHGvLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "08cfd225-d512-4c11-8b69-d0ce707fd7f0"
      },
      "source": [
        "# Connect to GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGJW8ZLIJQ_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "83c2572a-87ca-4cfa-8483-446f05462485"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 4.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 20.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=60ae511e3140ebf65e68a65541045b464fc8f11876a68056a4cb04b1ee096961\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjO9g_QsGt6d",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACo0x02IBfXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train_with_split.csv')\n",
        "#the empty choice is converted to a NaN when I reload, so this will correct the issue.\n",
        "df['a'].fillna(\"\", inplace=True)\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "mini_df = df.iloc[0:1000]\n",
        "mini_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oEo9ZfxK6JB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "outputId": "64c9b5b8-446a-417e-d1a1-f217191640eb"
      },
      "source": [
        "# ------ FOR A MINI TRAINING SET ------\n",
        "\n",
        "#Get the lists of sentences and their labels.\n",
        "contexts = mini_df.context.values\n",
        "questions = mini_df.question.values\n",
        "choices = mini_df[['a','b','c','d','e']].values\n",
        "#now converted to an INT\n",
        "mini_df.correct_index = mini_df.correct_index.fillna(0)\n",
        "labels = mini_df.correct_index.astype(int).values\n",
        "\n",
        "\n",
        "# ------ FOR THE FULL  TRAINING SET ------\n",
        "# contexts = df.context.values\n",
        "# questions = df.question.values\n",
        "# choices = df[['a','b','c','d','e']].values\n",
        "# #now converted to an INT\n",
        "# df.correct_index = df.correct_index.fillna(0)\n",
        "# labels = df.correct_index.astype(int).values\n",
        "\n",
        "print(labels.shape)\n",
        "print(torch.tensor(labels).unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000,)\n",
            "tensor([[3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 0, 1, 1, 4, 1, 1, 1,\n",
            "         2, 1, 4, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 4, 1,\n",
            "         0, 1, 1, 1, 1, 3, 1, 1, 2, 1, 2, 1, 0, 2, 4, 1, 1, 1, 2, 1, 0, 1, 1, 4,\n",
            "         0, 1, 0, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 3, 1, 1, 0, 2, 2, 1, 2, 1, 0, 1,\n",
            "         1, 1, 0, 2, 0, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 2, 3, 2, 0, 1, 1, 4, 1, 1,\n",
            "         1, 1, 1, 1, 0, 1, 1, 4, 4, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3, 1, 4, 2, 0, 1,\n",
            "         1, 3, 2, 0, 1, 1, 1, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 0,\n",
            "         1, 1, 1, 2, 1, 1, 4, 4, 1, 1, 1, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         3, 1, 2, 4, 1, 2, 1, 1, 1, 1, 2, 4, 0, 1, 2, 1, 4, 1, 0, 1, 2, 3, 1, 3,\n",
            "         2, 1, 1, 3, 2, 4, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1,\n",
            "         4, 1, 1, 1, 2, 4, 2, 1, 1, 1, 2, 1, 1, 0, 3, 1, 2, 1, 1, 3, 3, 1, 2, 1,\n",
            "         1, 2, 1, 3, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "         3, 1, 1, 0, 1, 1, 0, 0, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 3,\n",
            "         1, 1, 1, 2, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 4,\n",
            "         1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 4, 2, 0, 2, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1,\n",
            "         1, 0, 4, 3, 1, 0, 1, 1, 4, 1, 1, 1, 2, 1, 4, 1, 1, 0, 3, 4, 2, 0, 1, 1,\n",
            "         1, 1, 3, 1, 2, 1, 3, 4, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 4,\n",
            "         4, 1, 4, 1, 1, 3, 4, 0, 1, 1, 0, 2, 1, 3, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1,\n",
            "         1, 1, 1, 4, 1, 2, 1, 1, 4, 1, 1, 1, 1, 0, 4, 0, 3, 1, 1, 1, 1, 4, 2, 1,\n",
            "         3, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 2, 1, 0, 1, 2, 3, 1,\n",
            "         0, 1, 3, 1, 0, 4, 0, 0, 1, 4, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
            "         1, 2, 1, 1, 1, 2, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3,\n",
            "         3, 1, 2, 1, 1, 1, 3, 1, 1, 1, 3, 4, 1, 1, 1, 4, 0, 1, 1, 1, 1, 2, 0, 1,\n",
            "         1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "         2, 4, 2, 2, 0, 4, 1, 1, 1, 0, 3, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 2, 2, 1,\n",
            "         3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 4, 1, 3,\n",
            "         1, 0, 3, 0, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1, 0, 1,\n",
            "         0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2, 4, 1, 2, 1, 1,\n",
            "         1, 1, 1, 4, 2, 1, 1, 0, 2, 3, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 2, 4, 4, 2,\n",
            "         1, 1, 0, 3, 1, 1, 0, 4, 1, 3, 2, 1, 0, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1,\n",
            "         4, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 0, 1,\n",
            "         1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 4, 2,\n",
            "         1, 0, 1, 1, 1, 1, 0, 4, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "         1, 4, 1, 1, 2, 3, 1, 1, 2, 3, 3, 3, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 4, 1, 2, 1, 0, 1, 1, 0, 1, 0, 1, 2, 2, 4, 1, 0, 2, 1, 3,\n",
            "         1, 1, 3, 3, 2, 1, 1, 2, 1, 0, 3, 1, 1, 1, 4, 1, 1, 3, 1, 1, 3, 3, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "         1, 0, 1, 1, 2, 1, 4, 1, 1, 0, 1, 1, 1, 1, 1, 1]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb9nh89wGt9P",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Tokenize the Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3uzT8plGwH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "54ed402b73664159b46a865b33e8a780",
            "1835eb540e7749beb212253a34f16d61",
            "443bc3f343314b5d873563df4245ac09",
            "89e89d4486544a5d85697294eaa2f5ae",
            "55c02caa47ec435a8ecbec851c67ce62",
            "ad5f7b0914814b5395cbf34fcf5c5ea8",
            "b1f5fb87ebcc4d798a76c4c668755488",
            "b65c0d3d530d42f9a87c0e451028f856"
          ]
        },
        "outputId": "db82ab70-8527-4510-f4c6-a225515b7380"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54ed402b73664159b46a865b33e8a780",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpZjpKvSMPMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62ff5191-adab-4a46-dea7-824ea4bceda3"
      },
      "source": [
        "#Because the question and answer are combined, this may result\n",
        "#questions with greater than 512 tokens.\n",
        "\n",
        "max_len = 0\n",
        "for sent in contexts:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "#Quite a few errors here:  I will have to take the input length to max and truncate. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cskk7ZOfMPYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "\n",
        "# contexts = mini_df.context.values\n",
        "# questions = mini_df.question.values\n",
        "# choices = mini_df[['a','b','c','d','e']].values\n",
        "# labels = mini_df.correct_index.values\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "choices_features = []\n",
        "\n",
        "#---- THIS IS THE LOOP TO COMBINE THE QUESTIONS WITH THE CHOICES ----\n",
        "for i in range(len(questions)):\n",
        "    row = list(choices[i])\n",
        "    temp_list = []\n",
        "    for choice in row:\n",
        "      text = (str(questions[i])+' '+str(choice))\n",
        "      temp_list.append(text)\n",
        "\n",
        "    encoded_dict = tokenizer(\n",
        "                        [contexts[i],contexts[i],contexts[i], contexts[i], contexts[i]],\n",
        "                        temp_list,\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 384,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation = True)\n",
        "\n",
        "# ---- THIS IS THE LOOP TO COMBINE THE QUESTIONS WITH THE CONTEXTS ----   \n",
        "# for i in range(len(questions)):\n",
        "#     row = list(choices[i])\n",
        "#     temp_list = []\n",
        "#     new_question = (str(questions[i])+' '+str(contexts[i]))\n",
        "#     for choice in row:\n",
        "#       text = (str(choice))\n",
        "#       temp_list.append(text)\n",
        "\n",
        "#     encoded_dict = tokenizer(\n",
        "#                         [new_question,new_question,new_question,new_question,new_question],\n",
        "#                         temp_list,\n",
        "#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "#                         max_length = 384,           # Pad & truncate all sentences.\n",
        "#                         pad_to_max_length = True,\n",
        "#                         return_attention_mask = True,   # Construct attn. masks.\n",
        "#                         return_tensors = 'pt',\n",
        "#                         truncation = True)\n",
        "\n",
        "\n",
        "\n",
        "# #Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# # Convert the lists into tensors.\n",
        "#input_ids = torch.cat(input_ids, dim=0)\n",
        "input_ids = torch.stack(input_ids)\n",
        "attention_masks = torch.stack(attention_masks)\n",
        "labels = torch.tensor(labels).long()\n",
        "\n",
        "# # Print sentence 0, now as a list of IDs.\n",
        "# print('Original: ', contexts[0])\n",
        "# print('Token IDs:', input_ids[0])\n",
        "# print('Labels', labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HhwT3p29VXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fc161a8d-afa2-4712-8788-261581d8b264"
      },
      "source": [
        "# torch.save(input_ids, '/content/drive/My Drive/input_ids_384.pt')\n",
        "# torch.save(attention_masks, '/content/drive/My Drive/attn_mask_384.pt')\n",
        "# torch.save(labels, '/content/drive/My Drive/labels_384.pt')\n",
        "\n",
        "print(input_ids.size(0))\n",
        "print(attention_masks.size(0))\n",
        "print(labels.size(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "1000\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDwk0uNxxZBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c0b17447-b8e3-49a2-f07a-71476a9c8c87"
      },
      "source": [
        "input_ids = torch.load('/content/drive/My Drive/input_ids_384.pt')\n",
        "attention_masks = torch.load('/content/drive/My Drive/attn_mask_384.pt')\n",
        "labels = torch.load('/content/drive/My Drive/labels_384.pt')\n",
        "\n",
        "print(input_ids.size(0))\n",
        "print(attention_masks.size(0))\n",
        "print(labels.size(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "130319\n",
            "130319\n",
            "130319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcKXN6N2Q1c0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ac92cdbd-d59e-46f4-f942-cdf13890c249"
      },
      "source": [
        "# Going to do some prevalidation so I can watch the training loss\n",
        "# Before I run it on the dev set. \n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117,287 training samples\n",
            "13,032 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87q2tTIERXiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Up data Loader \n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 4\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size)\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQIH9OPrGwPq",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Load model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHs5MU2BGwXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "66bb522fa1d64b94abf09a8d117959ec",
            "34ecb434d71c447cbf4d691443a72fec",
            "aab834d947584b35b76f1932ddcc7b79",
            "c7f3fed236734147b32bf9d21e50d407",
            "8f2fe6f8811d4361a80842b47020072d",
            "d597b7c7d94746c7b052fc33cd11731b",
            "07718df366b3493c847903cd9534b929",
            "fc3170c6945143d7aa7305ffcf5d2a69",
            "915bff56c757455ba81723d09c3ecee2",
            "1058b8e4ab00420d8921bd8d62229e02",
            "7000da1013274b32b0d69c218f1532dd",
            "f963c35fc09c46caa3e6d79841d10e88",
            "28b61ef24ffc4560a94d560eea99a636",
            "65bc2805423e41caadbb532de85f783d",
            "71e944c28ac84f918f42ed77be82924d",
            "92252f2e128542b9a4868def2fdf70a9"
          ]
        },
        "outputId": "1a8770f8-fe22-436b-ac94-6b2565769eca"
      },
      "source": [
        "# Load the pretrained Bert Model for multiple choice. \n",
        " \n",
        "from transformers import BertForMultipleChoice, AdamW, BertConfig\n",
        "\n",
        "### NEED TO FIGURE OUT HOW TO TRAIN THIS MODEL FOR MULTIPLE CHOICE.\n",
        "model = BertForMultipleChoice.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels = 5,  \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66bb522fa1d64b94abf09a8d117959ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "915bff56c757455ba81723d09c3ecee2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultipleChoice(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TclciTB6QYRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set optimizer \n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate \n",
        "                  eps = 1e-8 # args.adam_epsilon  \n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilcv__q4QnwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 1\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_MKLf-2GweR",
        "colab_type": "text"
      },
      "source": [
        "### Step 6: Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNPEr_8VR-AZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "914a4e34-d1f9-45ab-e054-48194f62fe78"
      },
      "source": [
        "# Helper functions for training and timing.\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Format as hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GRqP3KDgXWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "47b08d09-7ee3-45a9-a021-cc29e9648e22"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=8371ad299b82d8a3b25759b229f315337a4098a67b064a33b64d1a457cbb373b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 24.2 GB  |     Proc size: 4.3 GB\n",
            "GPU RAM Free: 15015MB | Used: 1265MB | Util   8% | Total     16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSCwz9uPR-6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15936000-4756-4f9a-e4c8-bd6b90a45c9f"
      },
      "source": [
        "#Set Seed\n",
        "seed_val = 1\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "#Training Loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0 #Reset total loss. \n",
        "    model.train() #put model into training mode.\n",
        "\n",
        "    # Iterate through the batch.\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "    \n",
        "        model.zero_grad() #reset gradient       \n",
        "\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        total_train_loss += loss.item() #calc loss\n",
        "        loss.backward() #update gradients \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #clip the gradients\n",
        "        optimizer.step() #update parameters \n",
        "        scheduler.step() # Update the learning rate.\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # Validation\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval() #put the model in evaluation mode. \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        " \n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        total_eval_loss += loss.item() #calc loss\n",
        "        logits = logits.detach().cpu().numpy() # Move logits and labels to CPU\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids) # running accuracy\n",
        "        \n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch   100  of  29,322.    Elapsed: 0:01:15.\n",
            "  Batch   200  of  29,322.    Elapsed: 0:02:29.\n",
            "  Batch   300  of  29,322.    Elapsed: 0:03:43.\n",
            "  Batch   400  of  29,322.    Elapsed: 0:04:57.\n",
            "  Batch   500  of  29,322.    Elapsed: 0:06:12.\n",
            "  Batch   600  of  29,322.    Elapsed: 0:07:26.\n",
            "  Batch   700  of  29,322.    Elapsed: 0:08:40.\n",
            "  Batch   800  of  29,322.    Elapsed: 0:09:55.\n",
            "  Batch   900  of  29,322.    Elapsed: 0:11:09.\n",
            "  Batch 1,000  of  29,322.    Elapsed: 0:12:23.\n",
            "  Batch 1,100  of  29,322.    Elapsed: 0:13:38.\n",
            "  Batch 1,200  of  29,322.    Elapsed: 0:14:52.\n",
            "  Batch 1,300  of  29,322.    Elapsed: 0:16:06.\n",
            "  Batch 1,400  of  29,322.    Elapsed: 0:17:21.\n",
            "  Batch 1,500  of  29,322.    Elapsed: 0:18:35.\n",
            "  Batch 1,600  of  29,322.    Elapsed: 0:19:49.\n",
            "  Batch 1,700  of  29,322.    Elapsed: 0:21:03.\n",
            "  Batch 1,800  of  29,322.    Elapsed: 0:22:18.\n",
            "  Batch 1,900  of  29,322.    Elapsed: 0:23:32.\n",
            "  Batch 2,000  of  29,322.    Elapsed: 0:24:46.\n",
            "  Batch 2,100  of  29,322.    Elapsed: 0:26:01.\n",
            "  Batch 2,200  of  29,322.    Elapsed: 0:27:15.\n",
            "  Batch 2,300  of  29,322.    Elapsed: 0:28:29.\n",
            "  Batch 2,400  of  29,322.    Elapsed: 0:29:44.\n",
            "  Batch 2,500  of  29,322.    Elapsed: 0:30:58.\n",
            "  Batch 2,600  of  29,322.    Elapsed: 0:32:12.\n",
            "  Batch 2,700  of  29,322.    Elapsed: 0:33:27.\n",
            "  Batch 2,800  of  29,322.    Elapsed: 0:34:41.\n",
            "  Batch 2,900  of  29,322.    Elapsed: 0:35:55.\n",
            "  Batch 3,000  of  29,322.    Elapsed: 0:37:10.\n",
            "  Batch 3,100  of  29,322.    Elapsed: 0:38:24.\n",
            "  Batch 3,200  of  29,322.    Elapsed: 0:39:39.\n",
            "  Batch 3,300  of  29,322.    Elapsed: 0:40:53.\n",
            "  Batch 3,400  of  29,322.    Elapsed: 0:42:07.\n",
            "  Batch 3,500  of  29,322.    Elapsed: 0:43:21.\n",
            "  Batch 3,600  of  29,322.    Elapsed: 0:44:36.\n",
            "  Batch 3,700  of  29,322.    Elapsed: 0:45:50.\n",
            "  Batch 3,800  of  29,322.    Elapsed: 0:47:04.\n",
            "  Batch 3,900  of  29,322.    Elapsed: 0:48:19.\n",
            "  Batch 4,000  of  29,322.    Elapsed: 0:49:33.\n",
            "  Batch 4,100  of  29,322.    Elapsed: 0:50:47.\n",
            "  Batch 4,200  of  29,322.    Elapsed: 0:52:02.\n",
            "  Batch 4,300  of  29,322.    Elapsed: 0:53:16.\n",
            "  Batch 4,400  of  29,322.    Elapsed: 0:54:30.\n",
            "  Batch 4,500  of  29,322.    Elapsed: 0:55:45.\n",
            "  Batch 4,600  of  29,322.    Elapsed: 0:56:59.\n",
            "  Batch 4,700  of  29,322.    Elapsed: 0:58:13.\n",
            "  Batch 4,800  of  29,322.    Elapsed: 0:59:28.\n",
            "  Batch 4,900  of  29,322.    Elapsed: 1:00:42.\n",
            "  Batch 5,000  of  29,322.    Elapsed: 1:01:56.\n",
            "  Batch 5,100  of  29,322.    Elapsed: 1:03:11.\n",
            "  Batch 5,200  of  29,322.    Elapsed: 1:04:25.\n",
            "  Batch 5,300  of  29,322.    Elapsed: 1:05:40.\n",
            "  Batch 5,400  of  29,322.    Elapsed: 1:06:54.\n",
            "  Batch 5,500  of  29,322.    Elapsed: 1:08:08.\n",
            "  Batch 5,600  of  29,322.    Elapsed: 1:09:23.\n",
            "  Batch 5,700  of  29,322.    Elapsed: 1:10:37.\n",
            "  Batch 5,800  of  29,322.    Elapsed: 1:11:51.\n",
            "  Batch 5,900  of  29,322.    Elapsed: 1:13:06.\n",
            "  Batch 6,000  of  29,322.    Elapsed: 1:14:20.\n",
            "  Batch 6,100  of  29,322.    Elapsed: 1:15:34.\n",
            "  Batch 6,200  of  29,322.    Elapsed: 1:16:49.\n",
            "  Batch 6,300  of  29,322.    Elapsed: 1:18:03.\n",
            "  Batch 6,400  of  29,322.    Elapsed: 1:19:17.\n",
            "  Batch 6,500  of  29,322.    Elapsed: 1:20:32.\n",
            "  Batch 6,600  of  29,322.    Elapsed: 1:21:46.\n",
            "  Batch 6,700  of  29,322.    Elapsed: 1:23:00.\n",
            "  Batch 6,800  of  29,322.    Elapsed: 1:24:15.\n",
            "  Batch 6,900  of  29,322.    Elapsed: 1:25:29.\n",
            "  Batch 7,000  of  29,322.    Elapsed: 1:26:43.\n",
            "  Batch 7,100  of  29,322.    Elapsed: 1:27:58.\n",
            "  Batch 7,200  of  29,322.    Elapsed: 1:29:12.\n",
            "  Batch 7,300  of  29,322.    Elapsed: 1:30:26.\n",
            "  Batch 7,400  of  29,322.    Elapsed: 1:31:40.\n",
            "  Batch 7,500  of  29,322.    Elapsed: 1:32:55.\n",
            "  Batch 7,600  of  29,322.    Elapsed: 1:34:09.\n",
            "  Batch 7,700  of  29,322.    Elapsed: 1:35:23.\n",
            "  Batch 7,800  of  29,322.    Elapsed: 1:36:38.\n",
            "  Batch 7,900  of  29,322.    Elapsed: 1:37:52.\n",
            "  Batch 8,000  of  29,322.    Elapsed: 1:39:06.\n",
            "  Batch 8,100  of  29,322.    Elapsed: 1:40:20.\n",
            "  Batch 8,200  of  29,322.    Elapsed: 1:41:35.\n",
            "  Batch 8,300  of  29,322.    Elapsed: 1:42:49.\n",
            "  Batch 8,400  of  29,322.    Elapsed: 1:44:03.\n",
            "  Batch 8,500  of  29,322.    Elapsed: 1:45:17.\n",
            "  Batch 8,600  of  29,322.    Elapsed: 1:46:32.\n",
            "  Batch 8,700  of  29,322.    Elapsed: 1:47:46.\n",
            "  Batch 8,800  of  29,322.    Elapsed: 1:49:00.\n",
            "  Batch 8,900  of  29,322.    Elapsed: 1:50:15.\n",
            "  Batch 9,000  of  29,322.    Elapsed: 1:51:29.\n",
            "  Batch 9,100  of  29,322.    Elapsed: 1:52:43.\n",
            "  Batch 9,200  of  29,322.    Elapsed: 1:53:57.\n",
            "  Batch 9,300  of  29,322.    Elapsed: 1:55:12.\n",
            "  Batch 9,400  of  29,322.    Elapsed: 1:56:26.\n",
            "  Batch 9,500  of  29,322.    Elapsed: 1:57:40.\n",
            "  Batch 9,600  of  29,322.    Elapsed: 1:58:55.\n",
            "  Batch 9,700  of  29,322.    Elapsed: 2:00:09.\n",
            "  Batch 9,800  of  29,322.    Elapsed: 2:01:23.\n",
            "  Batch 9,900  of  29,322.    Elapsed: 2:02:37.\n",
            "  Batch 10,000  of  29,322.    Elapsed: 2:03:52.\n",
            "  Batch 10,100  of  29,322.    Elapsed: 2:05:06.\n",
            "  Batch 10,200  of  29,322.    Elapsed: 2:06:20.\n",
            "  Batch 10,300  of  29,322.    Elapsed: 2:07:35.\n",
            "  Batch 10,400  of  29,322.    Elapsed: 2:08:49.\n",
            "  Batch 10,500  of  29,322.    Elapsed: 2:10:03.\n",
            "  Batch 10,600  of  29,322.    Elapsed: 2:11:17.\n",
            "  Batch 10,700  of  29,322.    Elapsed: 2:12:32.\n",
            "  Batch 10,800  of  29,322.    Elapsed: 2:13:46.\n",
            "  Batch 10,900  of  29,322.    Elapsed: 2:15:00.\n",
            "  Batch 11,000  of  29,322.    Elapsed: 2:16:15.\n",
            "  Batch 11,100  of  29,322.    Elapsed: 2:17:29.\n",
            "  Batch 11,200  of  29,322.    Elapsed: 2:18:43.\n",
            "  Batch 11,300  of  29,322.    Elapsed: 2:19:58.\n",
            "  Batch 11,400  of  29,322.    Elapsed: 2:21:12.\n",
            "  Batch 11,500  of  29,322.    Elapsed: 2:22:26.\n",
            "  Batch 11,600  of  29,322.    Elapsed: 2:23:41.\n",
            "  Batch 11,700  of  29,322.    Elapsed: 2:24:55.\n",
            "  Batch 11,800  of  29,322.    Elapsed: 2:26:09.\n",
            "  Batch 11,900  of  29,322.    Elapsed: 2:27:24.\n",
            "  Batch 12,000  of  29,322.    Elapsed: 2:28:38.\n",
            "  Batch 12,100  of  29,322.    Elapsed: 2:29:52.\n",
            "  Batch 12,200  of  29,322.    Elapsed: 2:31:07.\n",
            "  Batch 12,300  of  29,322.    Elapsed: 2:32:21.\n",
            "  Batch 12,400  of  29,322.    Elapsed: 2:33:36.\n",
            "  Batch 12,500  of  29,322.    Elapsed: 2:34:50.\n",
            "  Batch 12,600  of  29,322.    Elapsed: 2:36:04.\n",
            "  Batch 12,700  of  29,322.    Elapsed: 2:37:19.\n",
            "  Batch 12,800  of  29,322.    Elapsed: 2:38:33.\n",
            "  Batch 12,900  of  29,322.    Elapsed: 2:39:47.\n",
            "  Batch 13,000  of  29,322.    Elapsed: 2:41:02.\n",
            "  Batch 13,100  of  29,322.    Elapsed: 2:42:16.\n",
            "  Batch 13,200  of  29,322.    Elapsed: 2:43:30.\n",
            "  Batch 13,300  of  29,322.    Elapsed: 2:44:45.\n",
            "  Batch 13,400  of  29,322.    Elapsed: 2:45:59.\n",
            "  Batch 13,500  of  29,322.    Elapsed: 2:47:13.\n",
            "  Batch 13,600  of  29,322.    Elapsed: 2:48:28.\n",
            "  Batch 13,700  of  29,322.    Elapsed: 2:49:42.\n",
            "  Batch 13,800  of  29,322.    Elapsed: 2:50:56.\n",
            "  Batch 13,900  of  29,322.    Elapsed: 2:52:11.\n",
            "  Batch 14,000  of  29,322.    Elapsed: 2:53:25.\n",
            "  Batch 14,100  of  29,322.    Elapsed: 2:54:39.\n",
            "  Batch 14,200  of  29,322.    Elapsed: 2:55:54.\n",
            "  Batch 14,300  of  29,322.    Elapsed: 2:57:08.\n",
            "  Batch 14,400  of  29,322.    Elapsed: 2:58:23.\n",
            "  Batch 14,500  of  29,322.    Elapsed: 2:59:37.\n",
            "  Batch 14,600  of  29,322.    Elapsed: 3:00:51.\n",
            "  Batch 14,700  of  29,322.    Elapsed: 3:02:06.\n",
            "  Batch 14,800  of  29,322.    Elapsed: 3:03:20.\n",
            "  Batch 14,900  of  29,322.    Elapsed: 3:04:34.\n",
            "  Batch 15,000  of  29,322.    Elapsed: 3:05:48.\n",
            "  Batch 15,100  of  29,322.    Elapsed: 3:07:03.\n",
            "  Batch 15,200  of  29,322.    Elapsed: 3:08:17.\n",
            "  Batch 15,300  of  29,322.    Elapsed: 3:09:32.\n",
            "  Batch 15,400  of  29,322.    Elapsed: 3:10:46.\n",
            "  Batch 15,500  of  29,322.    Elapsed: 3:12:00.\n",
            "  Batch 15,600  of  29,322.    Elapsed: 3:13:15.\n",
            "  Batch 15,700  of  29,322.    Elapsed: 3:14:29.\n",
            "  Batch 15,800  of  29,322.    Elapsed: 3:15:43.\n",
            "  Batch 15,900  of  29,322.    Elapsed: 3:16:58.\n",
            "  Batch 16,000  of  29,322.    Elapsed: 3:18:12.\n",
            "  Batch 16,100  of  29,322.    Elapsed: 3:19:27.\n",
            "  Batch 16,200  of  29,322.    Elapsed: 3:20:41.\n",
            "  Batch 16,300  of  29,322.    Elapsed: 3:21:55.\n",
            "  Batch 16,400  of  29,322.    Elapsed: 3:23:10.\n",
            "  Batch 16,500  of  29,322.    Elapsed: 3:24:24.\n",
            "  Batch 16,600  of  29,322.    Elapsed: 3:25:38.\n",
            "  Batch 16,700  of  29,322.    Elapsed: 3:26:53.\n",
            "  Batch 16,800  of  29,322.    Elapsed: 3:28:07.\n",
            "  Batch 16,900  of  29,322.    Elapsed: 3:29:21.\n",
            "  Batch 17,000  of  29,322.    Elapsed: 3:30:36.\n",
            "  Batch 17,100  of  29,322.    Elapsed: 3:31:50.\n",
            "  Batch 17,200  of  29,322.    Elapsed: 3:33:05.\n",
            "  Batch 17,300  of  29,322.    Elapsed: 3:34:19.\n",
            "  Batch 17,400  of  29,322.    Elapsed: 3:35:33.\n",
            "  Batch 17,500  of  29,322.    Elapsed: 3:36:48.\n",
            "  Batch 17,600  of  29,322.    Elapsed: 3:38:02.\n",
            "  Batch 17,700  of  29,322.    Elapsed: 3:39:16.\n",
            "  Batch 17,800  of  29,322.    Elapsed: 3:40:31.\n",
            "  Batch 17,900  of  29,322.    Elapsed: 3:41:45.\n",
            "  Batch 18,000  of  29,322.    Elapsed: 3:42:59.\n",
            "  Batch 18,100  of  29,322.    Elapsed: 3:44:14.\n",
            "  Batch 18,200  of  29,322.    Elapsed: 3:45:28.\n",
            "  Batch 18,300  of  29,322.    Elapsed: 3:46:42.\n",
            "  Batch 18,400  of  29,322.    Elapsed: 3:47:57.\n",
            "  Batch 18,500  of  29,322.    Elapsed: 3:49:11.\n",
            "  Batch 18,600  of  29,322.    Elapsed: 3:50:25.\n",
            "  Batch 18,700  of  29,322.    Elapsed: 3:51:40.\n",
            "  Batch 18,800  of  29,322.    Elapsed: 3:52:54.\n",
            "  Batch 18,900  of  29,322.    Elapsed: 3:54:08.\n",
            "  Batch 19,000  of  29,322.    Elapsed: 3:55:23.\n",
            "  Batch 19,100  of  29,322.    Elapsed: 3:56:37.\n",
            "  Batch 19,200  of  29,322.    Elapsed: 3:57:51.\n",
            "  Batch 19,300  of  29,322.    Elapsed: 3:59:06.\n",
            "  Batch 19,400  of  29,322.    Elapsed: 4:00:20.\n",
            "  Batch 19,500  of  29,322.    Elapsed: 4:01:34.\n",
            "  Batch 19,600  of  29,322.    Elapsed: 4:02:49.\n",
            "  Batch 19,700  of  29,322.    Elapsed: 4:04:03.\n",
            "  Batch 19,800  of  29,322.    Elapsed: 4:05:18.\n",
            "  Batch 19,900  of  29,322.    Elapsed: 4:06:32.\n",
            "  Batch 20,000  of  29,322.    Elapsed: 4:07:46.\n",
            "  Batch 20,100  of  29,322.    Elapsed: 4:09:01.\n",
            "  Batch 20,200  of  29,322.    Elapsed: 4:10:15.\n",
            "  Batch 20,300  of  29,322.    Elapsed: 4:11:30.\n",
            "  Batch 20,400  of  29,322.    Elapsed: 4:12:44.\n",
            "  Batch 20,500  of  29,322.    Elapsed: 4:13:58.\n",
            "  Batch 20,600  of  29,322.    Elapsed: 4:15:13.\n",
            "  Batch 20,700  of  29,322.    Elapsed: 4:16:27.\n",
            "  Batch 20,800  of  29,322.    Elapsed: 4:17:41.\n",
            "  Batch 20,900  of  29,322.    Elapsed: 4:18:56.\n",
            "  Batch 21,000  of  29,322.    Elapsed: 4:20:10.\n",
            "  Batch 21,100  of  29,322.    Elapsed: 4:21:25.\n",
            "  Batch 21,200  of  29,322.    Elapsed: 4:22:39.\n",
            "  Batch 21,300  of  29,322.    Elapsed: 4:23:53.\n",
            "  Batch 21,400  of  29,322.    Elapsed: 4:25:08.\n",
            "  Batch 21,500  of  29,322.    Elapsed: 4:26:22.\n",
            "  Batch 21,600  of  29,322.    Elapsed: 4:27:37.\n",
            "  Batch 21,700  of  29,322.    Elapsed: 4:28:51.\n",
            "  Batch 21,800  of  29,322.    Elapsed: 4:30:05.\n",
            "  Batch 21,900  of  29,322.    Elapsed: 4:31:20.\n",
            "  Batch 22,000  of  29,322.    Elapsed: 4:32:34.\n",
            "  Batch 22,100  of  29,322.    Elapsed: 4:33:49.\n",
            "  Batch 22,200  of  29,322.    Elapsed: 4:35:03.\n",
            "  Batch 22,300  of  29,322.    Elapsed: 4:36:17.\n",
            "  Batch 22,400  of  29,322.    Elapsed: 4:37:32.\n",
            "  Batch 22,500  of  29,322.    Elapsed: 4:38:46.\n",
            "  Batch 22,600  of  29,322.    Elapsed: 4:40:00.\n",
            "  Batch 22,700  of  29,322.    Elapsed: 4:41:15.\n",
            "  Batch 22,800  of  29,322.    Elapsed: 4:42:29.\n",
            "  Batch 22,900  of  29,322.    Elapsed: 4:43:44.\n",
            "  Batch 23,000  of  29,322.    Elapsed: 4:44:58.\n",
            "  Batch 23,100  of  29,322.    Elapsed: 4:46:13.\n",
            "  Batch 23,200  of  29,322.    Elapsed: 4:47:27.\n",
            "  Batch 23,300  of  29,322.    Elapsed: 4:48:41.\n",
            "  Batch 23,400  of  29,322.    Elapsed: 4:49:56.\n",
            "  Batch 23,500  of  29,322.    Elapsed: 4:51:10.\n",
            "  Batch 23,600  of  29,322.    Elapsed: 4:52:25.\n",
            "  Batch 23,700  of  29,322.    Elapsed: 4:53:39.\n",
            "  Batch 23,800  of  29,322.    Elapsed: 4:54:53.\n",
            "  Batch 23,900  of  29,322.    Elapsed: 4:56:08.\n",
            "  Batch 24,000  of  29,322.    Elapsed: 4:57:22.\n",
            "  Batch 24,100  of  29,322.    Elapsed: 4:58:36.\n",
            "  Batch 24,200  of  29,322.    Elapsed: 4:59:51.\n",
            "  Batch 24,300  of  29,322.    Elapsed: 5:01:05.\n",
            "  Batch 24,400  of  29,322.    Elapsed: 5:02:20.\n",
            "  Batch 24,500  of  29,322.    Elapsed: 5:03:34.\n",
            "  Batch 24,600  of  29,322.    Elapsed: 5:04:48.\n",
            "  Batch 24,700  of  29,322.    Elapsed: 5:06:03.\n",
            "  Batch 24,800  of  29,322.    Elapsed: 5:07:17.\n",
            "  Batch 24,900  of  29,322.    Elapsed: 5:08:31.\n",
            "  Batch 25,000  of  29,322.    Elapsed: 5:09:46.\n",
            "  Batch 25,100  of  29,322.    Elapsed: 5:11:00.\n",
            "  Batch 25,200  of  29,322.    Elapsed: 5:12:15.\n",
            "  Batch 25,300  of  29,322.    Elapsed: 5:13:29.\n",
            "  Batch 25,400  of  29,322.    Elapsed: 5:14:43.\n",
            "  Batch 25,500  of  29,322.    Elapsed: 5:15:58.\n",
            "  Batch 25,600  of  29,322.    Elapsed: 5:17:12.\n",
            "  Batch 25,700  of  29,322.    Elapsed: 5:18:27.\n",
            "  Batch 25,800  of  29,322.    Elapsed: 5:19:41.\n",
            "  Batch 25,900  of  29,322.    Elapsed: 5:20:55.\n",
            "  Batch 26,000  of  29,322.    Elapsed: 5:22:10.\n",
            "  Batch 26,100  of  29,322.    Elapsed: 5:23:24.\n",
            "  Batch 26,200  of  29,322.    Elapsed: 5:24:38.\n",
            "  Batch 26,300  of  29,322.    Elapsed: 5:25:53.\n",
            "  Batch 26,400  of  29,322.    Elapsed: 5:27:07.\n",
            "  Batch 26,500  of  29,322.    Elapsed: 5:28:21.\n",
            "  Batch 26,600  of  29,322.    Elapsed: 5:29:36.\n",
            "  Batch 26,700  of  29,322.    Elapsed: 5:30:50.\n",
            "  Batch 26,800  of  29,322.    Elapsed: 5:32:04.\n",
            "  Batch 26,900  of  29,322.    Elapsed: 5:33:19.\n",
            "  Batch 27,000  of  29,322.    Elapsed: 5:34:33.\n",
            "  Batch 27,100  of  29,322.    Elapsed: 5:35:47.\n",
            "  Batch 27,200  of  29,322.    Elapsed: 5:37:02.\n",
            "  Batch 27,300  of  29,322.    Elapsed: 5:38:16.\n",
            "  Batch 27,400  of  29,322.    Elapsed: 5:39:30.\n",
            "  Batch 27,500  of  29,322.    Elapsed: 5:40:45.\n",
            "  Batch 27,600  of  29,322.    Elapsed: 5:41:59.\n",
            "  Batch 27,700  of  29,322.    Elapsed: 5:43:13.\n",
            "  Batch 27,800  of  29,322.    Elapsed: 5:44:28.\n",
            "  Batch 27,900  of  29,322.    Elapsed: 5:45:42.\n",
            "  Batch 28,000  of  29,322.    Elapsed: 5:46:56.\n",
            "  Batch 28,100  of  29,322.    Elapsed: 5:48:11.\n",
            "  Batch 28,200  of  29,322.    Elapsed: 5:49:25.\n",
            "  Batch 28,300  of  29,322.    Elapsed: 5:50:39.\n",
            "  Batch 28,400  of  29,322.    Elapsed: 5:51:54.\n",
            "  Batch 28,500  of  29,322.    Elapsed: 5:53:08.\n",
            "  Batch 28,600  of  29,322.    Elapsed: 5:54:22.\n",
            "  Batch 28,700  of  29,322.    Elapsed: 5:55:37.\n",
            "  Batch 28,800  of  29,322.    Elapsed: 5:56:51.\n",
            "  Batch 28,900  of  29,322.    Elapsed: 5:58:05.\n",
            "  Batch 29,000  of  29,322.    Elapsed: 5:59:20.\n",
            "  Batch 29,100  of  29,322.    Elapsed: 6:00:34.\n",
            "  Batch 29,200  of  29,322.    Elapsed: 6:01:48.\n",
            "  Batch 29,300  of  29,322.    Elapsed: 6:03:02.\n",
            "  Average training loss: 1.03\n",
            "  Training epcoh took: 6:03:19\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 0.89\n",
            "  Validation took: 0:13:03\n",
            "\n",
            "Training complete!\n",
            "Total training took 6:16:22 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlQ3dP6JWUMy",
        "colab_type": "text"
      },
      "source": [
        "### Step 7: Visualize Training Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD4C4QqXR-_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make a dataframe of results. \n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU00jJPFWZ6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot the results from the Dataframe\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj66rRTTXPB0",
        "colab_type": "text"
      },
      "source": [
        "### Step 8: Prep Dev Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7lRfm72X4VE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9d0522e0-0267-41aa-8408-2d0fac78738c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#import Dev set. \n",
        "dev_df = pd.read_csv('dev_with_split.csv')\n",
        "#the empty choice is converted to a NaN when I reload, so this will correct the issue.\n",
        "dev_df['a'].fillna(\"\", inplace=True)\n",
        "\n",
        "print('Number of dev sentences: {:,}\\n'.format(dev_df.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 11,873\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNTzPDlhfMNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pull out the relevant columns.\n",
        "\n",
        "contexts = dev_df.context.values\n",
        "questions = dev_df.question.values\n",
        "choices = dev_df[['a','b','c','d','e']].values\n",
        "#now converted to an INT\n",
        "dev_df.correct_index = dev_df.correct_index.fillna(1)\n",
        "labels = dev_df.correct_index.astype(int).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTxXkT3PemPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "choices_features = []\n",
        "\n",
        "#---- THIS IS THE LOOP TO COMBINE THE QUESTIONS WITH THE CHOICES ----\n",
        "for i in range(len(questions)):\n",
        "    row = list(choices[i])\n",
        "    temp_list = []\n",
        "    for choice in row:\n",
        "      text = (str(questions[i])+' '+str(choice))\n",
        "      temp_list.append(text)\n",
        "\n",
        "    encoded_dict = tokenizer(\n",
        "                        [contexts[i],contexts[i],contexts[i], contexts[i], contexts[i]],\n",
        "                        temp_list,\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 384,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation = True)\n",
        "\n",
        "# #Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# # Convert the lists into tensors.\n",
        "#input_ids = torch.cat(input_ids, dim=0)\n",
        "input_ids = torch.stack(input_ids)\n",
        "attention_masks = torch.stack(attention_masks)\n",
        "labels = torch.tensor(labels).long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Ye7Y4cf1GW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6db69a23-f975-47eb-ef19-8e08b8fd3ed9"
      },
      "source": [
        "#Check the shape to make sure it worked correctly. \n",
        "print(input_ids.size(0))\n",
        "print(attention_masks.size(0))\n",
        "print(labels.size(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11873\n",
            "11873\n",
            "11873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqVe8OfDemNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the batch size.  \n",
        "batch_size = 4  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNg5M2QNXXbU",
        "colab_type": "text"
      },
      "source": [
        "### Step 9: Evaluate Dev Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFbS9e0bX461",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a18414fd-dd44-4b84-9acd-b4b809580e6f"
      },
      "source": [
        "# Prediction on test set\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 11,873 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q40urDnUgYXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#len(true_labels)\n",
        "def get_label_list(true_labels):\n",
        "  full_label_list = []\n",
        "  for i in range(len(true_labels)):\n",
        "    for j in range(len(true_labels[i])):\n",
        "      full_label_list.append(true_labels[i][j])\n",
        "  return full_label_list\n",
        "\n",
        "full_labels = get_label_list(true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iks-HKOlgYjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "a6ee0298-1130-4afb-bca8-ef20924801d8"
      },
      "source": [
        "answer_df = dev_df[['a','b','c','d','e']]\n",
        "\n",
        "for i in range(10):\n",
        "  print(answer_df.iloc[i, full_labels[i]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "France\n",
            "10th and 11th centuries\n",
            "Denmark, Iceland and Norway\n",
            "Rollo\n",
            "10th century\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "William the Conqueror\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlLg09EbkiXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pred_dict(full_labels, df):\n",
        "    pred_dict = {}\n",
        "    for i in range(len(full_labels)):\n",
        "        key = str(dev_df['id'][i])\n",
        "        best_guess = str(df.iloc[i, full_labels[i]])\n",
        "        pred_dict[key] = best_guess\n",
        "    return pred_dict \n",
        "\n",
        "def output_predictions(predictions):\n",
        "    with open('preds.json', 'w', encoding = 'utf-8') as json_file:\n",
        "        json.dump(pred_dict, json_file, ensure_ascii=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxj7ioqTkiej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "answer_df = dev_df[['a','b','c','d','e']]\n",
        "\n",
        "pred_dict = get_pred_dict(full_labels, answer_df)\n",
        "output_predictions(pred_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0RC14KxpCzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save a copy in my drive.\n",
        "\n",
        "%cp -R /content/preds.json /content/drive/My\\ Drive/model_save "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxTBJ_gxpNBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone SQUAD repo for the evaluation file.\n",
        "# Move the eval file to my content folder \n",
        "\n",
        "!git clone https://github.com/white127/SQUAD-2.0-bidaf.git\n",
        "%mv /content/SQUAD-2.0-bidaf/evaluate-v2.0.py /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhqVVl0qqZYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "6b8c9976-cb50-4436-c7e9-81cd051b36c7"
      },
      "source": [
        "# Still download the Dev set.\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-17 21:24:10--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "\rdev-v2.0.json         0%[                    ]       0  --.-KB/s               \rdev-v2.0.json        90%[=================>  ]   3.77M  18.9MB/s               \rdev-v2.0.json       100%[===================>]   4.17M  19.9MB/s    in 0.2s    \n",
            "\n",
            "2020-07-17 21:24:10 (19.9 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1w2J0GtpNGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Results for SE-4, with 5 way Mutli Choice\")\n",
        "!python evaluate-v2.0.py dev-v2.0.json preds.json\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luE3KtO7Xb08",
        "colab_type": "text"
      },
      "source": [
        "### Step 10: Save Fine-Tuned Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISe-Pv-cW-LY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "552d5d98-ec17-493d-847a-bd4e758faf29"
      },
      "source": [
        "##### MAKE SURE YOU MOVE A COPY TO YOUR BUCKET.\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/model_save/model_state_dict.pth')\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "#torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7894d6e730d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Good practice: save your training arguments together with the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_args.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    }
  ]
}