{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SE4_AB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "O3CSRuKkGt2E",
        "hmN18vXMGt4N",
        "CjO9g_QsGt6d",
        "rb9nh89wGt9P",
        "rQIH9OPrGwPq",
        "o_MKLf-2GweR",
        "rlQ3dP6JWUMy",
        "sj66rRTTXPB0",
        "ZNg5M2QNXXbU",
        "luE3KtO7Xb08"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoWyMHHSGxLk",
        "colab_type": "text"
      },
      "source": [
        "# AB Model\n",
        "\n",
        "\n",
        "*   Take the train dataset generated from the 4 way self ensemble training and train a multi choice model with only 2 options\n",
        "*   Use Huggingface transformers: BertForMultipleChoice\n",
        "*   Will preprocess and batch the text\n",
        "*   Evaluate results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3CSRuKkGt2E",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: File set up. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su1DUrXVGu0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0e3e812d-3414-4a96-edfb-a425a1ddca50"
      },
      "source": [
        "#Mount my drive so that I can access the split training sets. \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpKvepHKHnBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#copy the training data to colab\n",
        "\n",
        "%cp -R /content/drive/My\\ Drive/train_AB.csv /content/\n",
        "%cp -R /content/drive/My\\ Drive/dev_AB.csv /content/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmN18vXMGt4N",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Set up GPU and HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdxA13PHGvLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "46991b85-e4fe-4e89-f0bb-955a011ca605"
      },
      "source": [
        "# Connect to GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGJW8ZLIJQ_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "a1ce0777-a7b2-4bb8-fbbb-198a76816f0b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjO9g_QsGt6d",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACo0x02IBfXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5febe9c0-922c-439a-df5c-649aff4f83ae"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train_AB.csv')\n",
        "#the empty choice is converted to a NaN when I reload, so this will correct the issue.\n",
        "df['a'].fillna(\"\", inplace=True)\n",
        "df['b'].fillna(\"_padding_\", inplace=True)\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# mini_df = df.iloc[0:1000]\n",
        "# mini_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 130,319\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oEo9ZfxK6JB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3de4f0af-f1b5-4b1f-98b2-ec26cdcb5c1c"
      },
      "source": [
        "# ------ FOR A MINI TRAINING SET ------\n",
        "\n",
        "# #Get the lists of sentences and their labels.\n",
        "# contexts = mini_df.context.values\n",
        "# questions = mini_df.question.values\n",
        "# choices = mini_df[['a','b']].values\n",
        "# #now converted to an INT\n",
        "# mini_df.correct_index = mini_df.correct_index.fillna(0)\n",
        "# labels = mini_df.correct_index.astype(int).values\n",
        "\n",
        "\n",
        "# ------ FOR THE FULL  TRAINING SET ------\n",
        "contexts = df.context.values\n",
        "questions = df.question.values\n",
        "choices = df[['a','b']].values\n",
        "#now converted to an INT\n",
        "df.correct_index = df.correct_index.fillna(0)\n",
        "labels = df.correct_index.astype(int).values\n",
        "\n",
        "print(labels.shape)\n",
        "#print(torch.tensor(labels).unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(130319,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb9nh89wGt9P",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Tokenize the Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3uzT8plGwH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpZjpKvSMPMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d28015d3-fceb-4270-e909-aed9515a4360"
      },
      "source": [
        "#Because the question and answer are combined, this may result\n",
        "#questions with greater than 512 tokens.\n",
        "\n",
        "max_len = 0\n",
        "for sent in contexts:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "#Quite a few errors here:  I will have to take the input length to max and truncate. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cskk7ZOfMPYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "choices_features = []\n",
        "\n",
        "#---- THIS IS THE LOOP TO COMBINE THE QUESTIONS WITH THE CHOICES ----\n",
        "for i in range(len(questions)):\n",
        "    row = list(choices[i])\n",
        "    q_text = str(questions[i])+' '+str(contexts[i])\n",
        "    temp_list = []\n",
        "    for choice in row:\n",
        "      text = (str(choice))\n",
        "      temp_list.append(text)\n",
        "\n",
        "    encoded_dict = tokenizer(\n",
        "                        [q_text,q_text],\n",
        "                        temp_list,\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 384,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation = True)\n",
        "\n",
        "# #Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# # Convert the lists into tensors.\n",
        "#input_ids = torch.cat(input_ids, dim=0)\n",
        "input_ids = torch.stack(input_ids)\n",
        "attention_masks = torch.stack(attention_masks)\n",
        "labels = torch.tensor(labels).long()\n",
        "\n",
        "# # Print sentence 0, now as a list of IDs.\n",
        "# print('Original: ', contexts[0])\n",
        "# print('Token IDs:', input_ids[0])\n",
        "# print('Labels', labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HhwT3p29VXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "06772f13-be88-4670-923c-be84e532d373"
      },
      "source": [
        "#saved tokenized embeddings\n",
        "torch.save(input_ids, '/content/drive/My Drive/input_ids_384_AB.pt')\n",
        "torch.save(attention_masks, '/content/drive/My Drive/attn_mask_384_AB.pt')\n",
        "torch.save(labels, '/content/drive/My Drive/labels_384_AB.pt')\n",
        "\n",
        "print(input_ids.size(0))\n",
        "print(attention_masks.size(0))\n",
        "print(labels.size(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "130319\n",
            "130319\n",
            "130319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDwk0uNxxZBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c0b17447-b8e3-49a2-f07a-71476a9c8c87"
      },
      "source": [
        "# input_ids = torch.load('/content/drive/My Drive/input_ids_384_AB.pt')\n",
        "# attention_masks = torch.load('/content/drive/My Drive/attn_mask_384_AB.pt')\n",
        "# labels = torch.load('/content/drive/My Drive/labels_384_AB.pt')\n",
        "\n",
        "print(input_ids.size(0))\n",
        "print(attention_masks.size(0))\n",
        "print(labels.size(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "130319\n",
            "130319\n",
            "130319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcKXN6N2Q1c0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3fad5e6f-92bd-4211-d892-579bdcf0e593"
      },
      "source": [
        "# Going to do some prevalidation so I can watch the training loss\n",
        "# Before I run it on the dev set. \n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117,287 training samples\n",
            "13,032 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87q2tTIERXiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Up data Loader \n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 4\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size)\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQIH9OPrGwPq",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Load model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHs5MU2BGwXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "969955aa-b811-4242-f02d-cccd2fda9833"
      },
      "source": [
        "# Load the pretrained Bert Model for multiple choice. \n",
        " \n",
        "from transformers import BertForMultipleChoice, AdamW, BertConfig\n",
        "\n",
        "### NEED TO FIGURE OUT HOW TO TRAIN THIS MODEL FOR MULTIPLE CHOICE.\n",
        "model = BertForMultipleChoice.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels = 2,  \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultipleChoice(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TclciTB6QYRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set optimizer \n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-5, # args.learning_rate \n",
        "                  eps = 1e-8 # args.adam_epsilon  \n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilcv__q4QnwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 2\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_MKLf-2GweR",
        "colab_type": "text"
      },
      "source": [
        "### Step 6: Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNPEr_8VR-AZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "af813baf-893c-4855-beaf-a64e2f803516"
      },
      "source": [
        "# Helper functions for training and timing.\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Format as hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GRqP3KDgXWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "47b08d09-7ee3-45a9-a021-cc29e9648e22"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=8371ad299b82d8a3b25759b229f315337a4098a67b064a33b64d1a457cbb373b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 24.2 GB  |     Proc size: 4.3 GB\n",
            "GPU RAM Free: 15015MB | Used: 1265MB | Util   8% | Total     16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSCwz9uPR-6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72de8006-f699-4be2-820d-fff778206981"
      },
      "source": [
        "#Set Seed\n",
        "seed_val = 1\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "#Training Loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0 #Reset total loss. \n",
        "    model.train() #put model into training mode.\n",
        "\n",
        "    # Iterate through the batch.\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "    \n",
        "        model.zero_grad() #reset gradient       \n",
        "\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        total_train_loss += loss.item() #calc loss\n",
        "        loss.backward() #update gradients \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #clip the gradients\n",
        "        optimizer.step() #update parameters \n",
        "        scheduler.step() # Update the learning rate.\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # Validation\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval() #put the model in evaluation mode. \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        " \n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        total_eval_loss += loss.item() #calc loss\n",
        "        logits = logits.detach().cpu().numpy() # Move logits and labels to CPU\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids) # running accuracy\n",
        "        \n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    50  of  29,322.    Elapsed: 0:00:17.\n",
            "  Batch   100  of  29,322.    Elapsed: 0:00:33.\n",
            "  Batch   150  of  29,322.    Elapsed: 0:00:50.\n",
            "  Batch   200  of  29,322.    Elapsed: 0:01:06.\n",
            "  Batch   250  of  29,322.    Elapsed: 0:01:23.\n",
            "  Batch   300  of  29,322.    Elapsed: 0:01:40.\n",
            "  Batch   350  of  29,322.    Elapsed: 0:01:56.\n",
            "  Batch   400  of  29,322.    Elapsed: 0:02:13.\n",
            "  Batch   450  of  29,322.    Elapsed: 0:02:29.\n",
            "  Batch   500  of  29,322.    Elapsed: 0:02:46.\n",
            "  Batch   550  of  29,322.    Elapsed: 0:03:03.\n",
            "  Batch   600  of  29,322.    Elapsed: 0:03:19.\n",
            "  Batch   650  of  29,322.    Elapsed: 0:03:36.\n",
            "  Batch   700  of  29,322.    Elapsed: 0:03:52.\n",
            "  Batch   750  of  29,322.    Elapsed: 0:04:09.\n",
            "  Batch   800  of  29,322.    Elapsed: 0:04:25.\n",
            "  Batch   850  of  29,322.    Elapsed: 0:04:42.\n",
            "  Batch   900  of  29,322.    Elapsed: 0:04:59.\n",
            "  Batch   950  of  29,322.    Elapsed: 0:05:15.\n",
            "  Batch 1,000  of  29,322.    Elapsed: 0:05:32.\n",
            "  Batch 1,050  of  29,322.    Elapsed: 0:05:48.\n",
            "  Batch 1,100  of  29,322.    Elapsed: 0:06:05.\n",
            "  Batch 1,150  of  29,322.    Elapsed: 0:06:21.\n",
            "  Batch 1,200  of  29,322.    Elapsed: 0:06:38.\n",
            "  Batch 1,250  of  29,322.    Elapsed: 0:06:55.\n",
            "  Batch 1,300  of  29,322.    Elapsed: 0:07:11.\n",
            "  Batch 1,350  of  29,322.    Elapsed: 0:07:28.\n",
            "  Batch 1,400  of  29,322.    Elapsed: 0:07:44.\n",
            "  Batch 1,450  of  29,322.    Elapsed: 0:08:01.\n",
            "  Batch 1,500  of  29,322.    Elapsed: 0:08:17.\n",
            "  Batch 1,550  of  29,322.    Elapsed: 0:08:34.\n",
            "  Batch 1,600  of  29,322.    Elapsed: 0:08:50.\n",
            "  Batch 1,650  of  29,322.    Elapsed: 0:09:07.\n",
            "  Batch 1,700  of  29,322.    Elapsed: 0:09:24.\n",
            "  Batch 1,750  of  29,322.    Elapsed: 0:09:40.\n",
            "  Batch 1,800  of  29,322.    Elapsed: 0:09:57.\n",
            "  Batch 1,850  of  29,322.    Elapsed: 0:10:13.\n",
            "  Batch 1,900  of  29,322.    Elapsed: 0:10:30.\n",
            "  Batch 1,950  of  29,322.    Elapsed: 0:10:46.\n",
            "  Batch 2,000  of  29,322.    Elapsed: 0:11:03.\n",
            "  Batch 2,050  of  29,322.    Elapsed: 0:11:20.\n",
            "  Batch 2,100  of  29,322.    Elapsed: 0:11:36.\n",
            "  Batch 2,150  of  29,322.    Elapsed: 0:11:53.\n",
            "  Batch 2,200  of  29,322.    Elapsed: 0:12:09.\n",
            "  Batch 2,250  of  29,322.    Elapsed: 0:12:26.\n",
            "  Batch 2,300  of  29,322.    Elapsed: 0:12:42.\n",
            "  Batch 2,350  of  29,322.    Elapsed: 0:12:59.\n",
            "  Batch 2,400  of  29,322.    Elapsed: 0:13:16.\n",
            "  Batch 2,450  of  29,322.    Elapsed: 0:13:32.\n",
            "  Batch 2,500  of  29,322.    Elapsed: 0:13:49.\n",
            "  Batch 2,550  of  29,322.    Elapsed: 0:14:05.\n",
            "  Batch 2,600  of  29,322.    Elapsed: 0:14:22.\n",
            "  Batch 2,650  of  29,322.    Elapsed: 0:14:38.\n",
            "  Batch 2,700  of  29,322.    Elapsed: 0:14:55.\n",
            "  Batch 2,750  of  29,322.    Elapsed: 0:15:11.\n",
            "  Batch 2,800  of  29,322.    Elapsed: 0:15:28.\n",
            "  Batch 2,850  of  29,322.    Elapsed: 0:15:45.\n",
            "  Batch 2,900  of  29,322.    Elapsed: 0:16:01.\n",
            "  Batch 2,950  of  29,322.    Elapsed: 0:16:18.\n",
            "  Batch 3,000  of  29,322.    Elapsed: 0:16:35.\n",
            "  Batch 3,050  of  29,322.    Elapsed: 0:16:51.\n",
            "  Batch 3,100  of  29,322.    Elapsed: 0:17:08.\n",
            "  Batch 3,150  of  29,322.    Elapsed: 0:17:25.\n",
            "  Batch 3,200  of  29,322.    Elapsed: 0:17:41.\n",
            "  Batch 3,250  of  29,322.    Elapsed: 0:17:58.\n",
            "  Batch 3,300  of  29,322.    Elapsed: 0:18:15.\n",
            "  Batch 3,350  of  29,322.    Elapsed: 0:18:31.\n",
            "  Batch 3,400  of  29,322.    Elapsed: 0:18:48.\n",
            "  Batch 3,450  of  29,322.    Elapsed: 0:19:04.\n",
            "  Batch 3,500  of  29,322.    Elapsed: 0:19:21.\n",
            "  Batch 3,550  of  29,322.    Elapsed: 0:19:38.\n",
            "  Batch 3,600  of  29,322.    Elapsed: 0:19:54.\n",
            "  Batch 3,650  of  29,322.    Elapsed: 0:20:11.\n",
            "  Batch 3,700  of  29,322.    Elapsed: 0:20:28.\n",
            "  Batch 3,750  of  29,322.    Elapsed: 0:20:44.\n",
            "  Batch 3,800  of  29,322.    Elapsed: 0:21:01.\n",
            "  Batch 3,850  of  29,322.    Elapsed: 0:21:17.\n",
            "  Batch 3,900  of  29,322.    Elapsed: 0:21:34.\n",
            "  Batch 3,950  of  29,322.    Elapsed: 0:21:51.\n",
            "  Batch 4,000  of  29,322.    Elapsed: 0:22:07.\n",
            "  Batch 4,050  of  29,322.    Elapsed: 0:22:24.\n",
            "  Batch 4,100  of  29,322.    Elapsed: 0:22:41.\n",
            "  Batch 4,150  of  29,322.    Elapsed: 0:22:57.\n",
            "  Batch 4,200  of  29,322.    Elapsed: 0:23:14.\n",
            "  Batch 4,250  of  29,322.    Elapsed: 0:23:30.\n",
            "  Batch 4,300  of  29,322.    Elapsed: 0:23:47.\n",
            "  Batch 4,350  of  29,322.    Elapsed: 0:24:03.\n",
            "  Batch 4,400  of  29,322.    Elapsed: 0:24:20.\n",
            "  Batch 4,450  of  29,322.    Elapsed: 0:24:37.\n",
            "  Batch 4,500  of  29,322.    Elapsed: 0:24:53.\n",
            "  Batch 4,550  of  29,322.    Elapsed: 0:25:10.\n",
            "  Batch 4,600  of  29,322.    Elapsed: 0:25:26.\n",
            "  Batch 4,650  of  29,322.    Elapsed: 0:25:43.\n",
            "  Batch 4,700  of  29,322.    Elapsed: 0:26:00.\n",
            "  Batch 4,750  of  29,322.    Elapsed: 0:26:16.\n",
            "  Batch 4,800  of  29,322.    Elapsed: 0:26:33.\n",
            "  Batch 4,850  of  29,322.    Elapsed: 0:26:49.\n",
            "  Batch 4,900  of  29,322.    Elapsed: 0:27:06.\n",
            "  Batch 4,950  of  29,322.    Elapsed: 0:27:23.\n",
            "  Batch 5,000  of  29,322.    Elapsed: 0:27:39.\n",
            "  Batch 5,050  of  29,322.    Elapsed: 0:27:56.\n",
            "  Batch 5,100  of  29,322.    Elapsed: 0:28:12.\n",
            "  Batch 5,150  of  29,322.    Elapsed: 0:28:29.\n",
            "  Batch 5,200  of  29,322.    Elapsed: 0:28:46.\n",
            "  Batch 5,250  of  29,322.    Elapsed: 0:29:02.\n",
            "  Batch 5,300  of  29,322.    Elapsed: 0:29:19.\n",
            "  Batch 5,350  of  29,322.    Elapsed: 0:29:35.\n",
            "  Batch 5,400  of  29,322.    Elapsed: 0:29:52.\n",
            "  Batch 5,450  of  29,322.    Elapsed: 0:30:09.\n",
            "  Batch 5,500  of  29,322.    Elapsed: 0:30:25.\n",
            "  Batch 5,550  of  29,322.    Elapsed: 0:30:42.\n",
            "  Batch 5,600  of  29,322.    Elapsed: 0:30:58.\n",
            "  Batch 5,650  of  29,322.    Elapsed: 0:31:15.\n",
            "  Batch 5,700  of  29,322.    Elapsed: 0:31:32.\n",
            "  Batch 5,750  of  29,322.    Elapsed: 0:31:48.\n",
            "  Batch 5,800  of  29,322.    Elapsed: 0:32:05.\n",
            "  Batch 5,850  of  29,322.    Elapsed: 0:32:22.\n",
            "  Batch 5,900  of  29,322.    Elapsed: 0:32:38.\n",
            "  Batch 5,950  of  29,322.    Elapsed: 0:32:55.\n",
            "  Batch 6,000  of  29,322.    Elapsed: 0:33:11.\n",
            "  Batch 6,050  of  29,322.    Elapsed: 0:33:28.\n",
            "  Batch 6,100  of  29,322.    Elapsed: 0:33:45.\n",
            "  Batch 6,150  of  29,322.    Elapsed: 0:34:01.\n",
            "  Batch 6,200  of  29,322.    Elapsed: 0:34:18.\n",
            "  Batch 6,250  of  29,322.    Elapsed: 0:34:34.\n",
            "  Batch 6,300  of  29,322.    Elapsed: 0:34:51.\n",
            "  Batch 6,350  of  29,322.    Elapsed: 0:35:08.\n",
            "  Batch 6,400  of  29,322.    Elapsed: 0:35:24.\n",
            "  Batch 6,450  of  29,322.    Elapsed: 0:35:41.\n",
            "  Batch 6,500  of  29,322.    Elapsed: 0:35:57.\n",
            "  Batch 6,550  of  29,322.    Elapsed: 0:36:14.\n",
            "  Batch 6,600  of  29,322.    Elapsed: 0:36:30.\n",
            "  Batch 6,650  of  29,322.    Elapsed: 0:36:47.\n",
            "  Batch 6,700  of  29,322.    Elapsed: 0:37:04.\n",
            "  Batch 6,750  of  29,322.    Elapsed: 0:37:20.\n",
            "  Batch 6,800  of  29,322.    Elapsed: 0:37:37.\n",
            "  Batch 6,850  of  29,322.    Elapsed: 0:37:53.\n",
            "  Batch 6,900  of  29,322.    Elapsed: 0:38:10.\n",
            "  Batch 6,950  of  29,322.    Elapsed: 0:38:27.\n",
            "  Batch 7,000  of  29,322.    Elapsed: 0:38:43.\n",
            "  Batch 7,050  of  29,322.    Elapsed: 0:39:00.\n",
            "  Batch 7,100  of  29,322.    Elapsed: 0:39:16.\n",
            "  Batch 7,150  of  29,322.    Elapsed: 0:39:33.\n",
            "  Batch 7,200  of  29,322.    Elapsed: 0:39:49.\n",
            "  Batch 7,250  of  29,322.    Elapsed: 0:40:06.\n",
            "  Batch 7,300  of  29,322.    Elapsed: 0:40:23.\n",
            "  Batch 7,350  of  29,322.    Elapsed: 0:40:39.\n",
            "  Batch 7,400  of  29,322.    Elapsed: 0:40:56.\n",
            "  Batch 7,450  of  29,322.    Elapsed: 0:41:12.\n",
            "  Batch 7,500  of  29,322.    Elapsed: 0:41:29.\n",
            "  Batch 7,550  of  29,322.    Elapsed: 0:41:45.\n",
            "  Batch 7,600  of  29,322.    Elapsed: 0:42:02.\n",
            "  Batch 7,650  of  29,322.    Elapsed: 0:42:19.\n",
            "  Batch 7,700  of  29,322.    Elapsed: 0:42:35.\n",
            "  Batch 7,750  of  29,322.    Elapsed: 0:42:52.\n",
            "  Batch 7,800  of  29,322.    Elapsed: 0:43:08.\n",
            "  Batch 7,850  of  29,322.    Elapsed: 0:43:25.\n",
            "  Batch 7,900  of  29,322.    Elapsed: 0:43:41.\n",
            "  Batch 7,950  of  29,322.    Elapsed: 0:43:58.\n",
            "  Batch 8,000  of  29,322.    Elapsed: 0:44:15.\n",
            "  Batch 8,050  of  29,322.    Elapsed: 0:44:31.\n",
            "  Batch 8,100  of  29,322.    Elapsed: 0:44:48.\n",
            "  Batch 8,150  of  29,322.    Elapsed: 0:45:04.\n",
            "  Batch 8,200  of  29,322.    Elapsed: 0:45:21.\n",
            "  Batch 8,250  of  29,322.    Elapsed: 0:45:38.\n",
            "  Batch 8,300  of  29,322.    Elapsed: 0:45:54.\n",
            "  Batch 8,350  of  29,322.    Elapsed: 0:46:11.\n",
            "  Batch 8,400  of  29,322.    Elapsed: 0:46:27.\n",
            "  Batch 8,450  of  29,322.    Elapsed: 0:46:44.\n",
            "  Batch 8,500  of  29,322.    Elapsed: 0:47:01.\n",
            "  Batch 8,550  of  29,322.    Elapsed: 0:47:17.\n",
            "  Batch 8,600  of  29,322.    Elapsed: 0:47:34.\n",
            "  Batch 8,650  of  29,322.    Elapsed: 0:47:50.\n",
            "  Batch 8,700  of  29,322.    Elapsed: 0:48:07.\n",
            "  Batch 8,750  of  29,322.    Elapsed: 0:48:23.\n",
            "  Batch 8,800  of  29,322.    Elapsed: 0:48:40.\n",
            "  Batch 8,850  of  29,322.    Elapsed: 0:48:57.\n",
            "  Batch 8,900  of  29,322.    Elapsed: 0:49:13.\n",
            "  Batch 8,950  of  29,322.    Elapsed: 0:49:30.\n",
            "  Batch 9,000  of  29,322.    Elapsed: 0:49:46.\n",
            "  Batch 9,050  of  29,322.    Elapsed: 0:50:03.\n",
            "  Batch 9,100  of  29,322.    Elapsed: 0:50:20.\n",
            "  Batch 9,150  of  29,322.    Elapsed: 0:50:36.\n",
            "  Batch 9,200  of  29,322.    Elapsed: 0:50:53.\n",
            "  Batch 9,250  of  29,322.    Elapsed: 0:51:09.\n",
            "  Batch 9,300  of  29,322.    Elapsed: 0:51:26.\n",
            "  Batch 9,350  of  29,322.    Elapsed: 0:51:42.\n",
            "  Batch 9,400  of  29,322.    Elapsed: 0:51:59.\n",
            "  Batch 9,450  of  29,322.    Elapsed: 0:52:15.\n",
            "  Batch 9,500  of  29,322.    Elapsed: 0:52:32.\n",
            "  Batch 9,550  of  29,322.    Elapsed: 0:52:49.\n",
            "  Batch 9,600  of  29,322.    Elapsed: 0:53:05.\n",
            "  Batch 9,650  of  29,322.    Elapsed: 0:53:22.\n",
            "  Batch 9,700  of  29,322.    Elapsed: 0:53:38.\n",
            "  Batch 9,750  of  29,322.    Elapsed: 0:53:55.\n",
            "  Batch 9,800  of  29,322.    Elapsed: 0:54:11.\n",
            "  Batch 9,850  of  29,322.    Elapsed: 0:54:28.\n",
            "  Batch 9,900  of  29,322.    Elapsed: 0:54:45.\n",
            "  Batch 9,950  of  29,322.    Elapsed: 0:55:01.\n",
            "  Batch 10,000  of  29,322.    Elapsed: 0:55:18.\n",
            "  Batch 10,050  of  29,322.    Elapsed: 0:55:34.\n",
            "  Batch 10,100  of  29,322.    Elapsed: 0:55:51.\n",
            "  Batch 10,150  of  29,322.    Elapsed: 0:56:07.\n",
            "  Batch 10,200  of  29,322.    Elapsed: 0:56:24.\n",
            "  Batch 10,250  of  29,322.    Elapsed: 0:56:41.\n",
            "  Batch 10,300  of  29,322.    Elapsed: 0:56:57.\n",
            "  Batch 10,350  of  29,322.    Elapsed: 0:57:14.\n",
            "  Batch 10,400  of  29,322.    Elapsed: 0:57:30.\n",
            "  Batch 10,450  of  29,322.    Elapsed: 0:57:47.\n",
            "  Batch 10,500  of  29,322.    Elapsed: 0:58:03.\n",
            "  Batch 10,550  of  29,322.    Elapsed: 0:58:20.\n",
            "  Batch 10,600  of  29,322.    Elapsed: 0:58:36.\n",
            "  Batch 10,650  of  29,322.    Elapsed: 0:58:53.\n",
            "  Batch 10,700  of  29,322.    Elapsed: 0:59:10.\n",
            "  Batch 10,750  of  29,322.    Elapsed: 0:59:26.\n",
            "  Batch 10,800  of  29,322.    Elapsed: 0:59:43.\n",
            "  Batch 10,850  of  29,322.    Elapsed: 0:59:59.\n",
            "  Batch 10,900  of  29,322.    Elapsed: 1:00:16.\n",
            "  Batch 10,950  of  29,322.    Elapsed: 1:00:32.\n",
            "  Batch 11,000  of  29,322.    Elapsed: 1:00:49.\n",
            "  Batch 11,050  of  29,322.    Elapsed: 1:01:06.\n",
            "  Batch 11,100  of  29,322.    Elapsed: 1:01:22.\n",
            "  Batch 11,150  of  29,322.    Elapsed: 1:01:39.\n",
            "  Batch 11,200  of  29,322.    Elapsed: 1:01:55.\n",
            "  Batch 11,250  of  29,322.    Elapsed: 1:02:12.\n",
            "  Batch 11,300  of  29,322.    Elapsed: 1:02:28.\n",
            "  Batch 11,350  of  29,322.    Elapsed: 1:02:45.\n",
            "  Batch 11,400  of  29,322.    Elapsed: 1:03:01.\n",
            "  Batch 11,450  of  29,322.    Elapsed: 1:03:18.\n",
            "  Batch 11,500  of  29,322.    Elapsed: 1:03:35.\n",
            "  Batch 11,550  of  29,322.    Elapsed: 1:03:51.\n",
            "  Batch 11,600  of  29,322.    Elapsed: 1:04:08.\n",
            "  Batch 11,650  of  29,322.    Elapsed: 1:04:24.\n",
            "  Batch 11,700  of  29,322.    Elapsed: 1:04:41.\n",
            "  Batch 11,750  of  29,322.    Elapsed: 1:04:57.\n",
            "  Batch 11,800  of  29,322.    Elapsed: 1:05:14.\n",
            "  Batch 11,850  of  29,322.    Elapsed: 1:05:30.\n",
            "  Batch 11,900  of  29,322.    Elapsed: 1:05:47.\n",
            "  Batch 11,950  of  29,322.    Elapsed: 1:06:04.\n",
            "  Batch 12,000  of  29,322.    Elapsed: 1:06:20.\n",
            "  Batch 12,050  of  29,322.    Elapsed: 1:06:37.\n",
            "  Batch 12,100  of  29,322.    Elapsed: 1:06:53.\n",
            "  Batch 12,150  of  29,322.    Elapsed: 1:07:10.\n",
            "  Batch 12,200  of  29,322.    Elapsed: 1:07:26.\n",
            "  Batch 12,250  of  29,322.    Elapsed: 1:07:43.\n",
            "  Batch 12,300  of  29,322.    Elapsed: 1:07:59.\n",
            "  Batch 12,350  of  29,322.    Elapsed: 1:08:16.\n",
            "  Batch 12,400  of  29,322.    Elapsed: 1:08:33.\n",
            "  Batch 12,450  of  29,322.    Elapsed: 1:08:49.\n",
            "  Batch 12,500  of  29,322.    Elapsed: 1:09:06.\n",
            "  Batch 12,550  of  29,322.    Elapsed: 1:09:22.\n",
            "  Batch 12,600  of  29,322.    Elapsed: 1:09:39.\n",
            "  Batch 12,650  of  29,322.    Elapsed: 1:09:55.\n",
            "  Batch 12,700  of  29,322.    Elapsed: 1:10:12.\n",
            "  Batch 12,750  of  29,322.    Elapsed: 1:10:28.\n",
            "  Batch 12,800  of  29,322.    Elapsed: 1:10:45.\n",
            "  Batch 12,850  of  29,322.    Elapsed: 1:11:01.\n",
            "  Batch 12,900  of  29,322.    Elapsed: 1:11:18.\n",
            "  Batch 12,950  of  29,322.    Elapsed: 1:11:35.\n",
            "  Batch 13,000  of  29,322.    Elapsed: 1:11:51.\n",
            "  Batch 13,050  of  29,322.    Elapsed: 1:12:08.\n",
            "  Batch 13,100  of  29,322.    Elapsed: 1:12:24.\n",
            "  Batch 13,150  of  29,322.    Elapsed: 1:12:41.\n",
            "  Batch 13,200  of  29,322.    Elapsed: 1:12:57.\n",
            "  Batch 13,250  of  29,322.    Elapsed: 1:13:14.\n",
            "  Batch 13,300  of  29,322.    Elapsed: 1:13:30.\n",
            "  Batch 13,350  of  29,322.    Elapsed: 1:13:47.\n",
            "  Batch 13,400  of  29,322.    Elapsed: 1:14:03.\n",
            "  Batch 13,450  of  29,322.    Elapsed: 1:14:20.\n",
            "  Batch 13,500  of  29,322.    Elapsed: 1:14:37.\n",
            "  Batch 13,550  of  29,322.    Elapsed: 1:14:53.\n",
            "  Batch 13,600  of  29,322.    Elapsed: 1:15:10.\n",
            "  Batch 13,650  of  29,322.    Elapsed: 1:15:26.\n",
            "  Batch 13,700  of  29,322.    Elapsed: 1:15:43.\n",
            "  Batch 13,750  of  29,322.    Elapsed: 1:15:59.\n",
            "  Batch 13,800  of  29,322.    Elapsed: 1:16:16.\n",
            "  Batch 13,850  of  29,322.    Elapsed: 1:16:32.\n",
            "  Batch 13,900  of  29,322.    Elapsed: 1:16:49.\n",
            "  Batch 13,950  of  29,322.    Elapsed: 1:17:06.\n",
            "  Batch 14,000  of  29,322.    Elapsed: 1:17:22.\n",
            "  Batch 14,050  of  29,322.    Elapsed: 1:17:39.\n",
            "  Batch 14,100  of  29,322.    Elapsed: 1:17:55.\n",
            "  Batch 14,150  of  29,322.    Elapsed: 1:18:12.\n",
            "  Batch 14,200  of  29,322.    Elapsed: 1:18:28.\n",
            "  Batch 14,250  of  29,322.    Elapsed: 1:18:45.\n",
            "  Batch 14,300  of  29,322.    Elapsed: 1:19:02.\n",
            "  Batch 14,350  of  29,322.    Elapsed: 1:19:18.\n",
            "  Batch 14,400  of  29,322.    Elapsed: 1:19:35.\n",
            "  Batch 14,450  of  29,322.    Elapsed: 1:19:52.\n",
            "  Batch 14,500  of  29,322.    Elapsed: 1:20:08.\n",
            "  Batch 14,550  of  29,322.    Elapsed: 1:20:25.\n",
            "  Batch 14,600  of  29,322.    Elapsed: 1:20:41.\n",
            "  Batch 14,650  of  29,322.    Elapsed: 1:20:58.\n",
            "  Batch 14,700  of  29,322.    Elapsed: 1:21:14.\n",
            "  Batch 14,750  of  29,322.    Elapsed: 1:21:31.\n",
            "  Batch 14,800  of  29,322.    Elapsed: 1:21:48.\n",
            "  Batch 14,850  of  29,322.    Elapsed: 1:22:04.\n",
            "  Batch 14,900  of  29,322.    Elapsed: 1:22:21.\n",
            "  Batch 14,950  of  29,322.    Elapsed: 1:22:37.\n",
            "  Batch 15,000  of  29,322.    Elapsed: 1:22:54.\n",
            "  Batch 15,050  of  29,322.    Elapsed: 1:23:10.\n",
            "  Batch 15,100  of  29,322.    Elapsed: 1:23:27.\n",
            "  Batch 15,150  of  29,322.    Elapsed: 1:23:44.\n",
            "  Batch 15,200  of  29,322.    Elapsed: 1:24:00.\n",
            "  Batch 15,250  of  29,322.    Elapsed: 1:24:17.\n",
            "  Batch 15,300  of  29,322.    Elapsed: 1:24:33.\n",
            "  Batch 15,350  of  29,322.    Elapsed: 1:24:50.\n",
            "  Batch 15,400  of  29,322.    Elapsed: 1:25:06.\n",
            "  Batch 15,450  of  29,322.    Elapsed: 1:25:23.\n",
            "  Batch 15,500  of  29,322.    Elapsed: 1:25:40.\n",
            "  Batch 15,550  of  29,322.    Elapsed: 1:25:56.\n",
            "  Batch 15,600  of  29,322.    Elapsed: 1:26:13.\n",
            "  Batch 15,650  of  29,322.    Elapsed: 1:26:29.\n",
            "  Batch 15,700  of  29,322.    Elapsed: 1:26:46.\n",
            "  Batch 15,750  of  29,322.    Elapsed: 1:27:02.\n",
            "  Batch 15,800  of  29,322.    Elapsed: 1:27:19.\n",
            "  Batch 15,850  of  29,322.    Elapsed: 1:27:36.\n",
            "  Batch 15,900  of  29,322.    Elapsed: 1:27:52.\n",
            "  Batch 15,950  of  29,322.    Elapsed: 1:28:09.\n",
            "  Batch 16,000  of  29,322.    Elapsed: 1:28:25.\n",
            "  Batch 16,050  of  29,322.    Elapsed: 1:28:42.\n",
            "  Batch 16,100  of  29,322.    Elapsed: 1:28:58.\n",
            "  Batch 16,150  of  29,322.    Elapsed: 1:29:15.\n",
            "  Batch 16,200  of  29,322.    Elapsed: 1:29:32.\n",
            "  Batch 16,250  of  29,322.    Elapsed: 1:29:48.\n",
            "  Batch 16,300  of  29,322.    Elapsed: 1:30:05.\n",
            "  Batch 16,350  of  29,322.    Elapsed: 1:30:21.\n",
            "  Batch 16,400  of  29,322.    Elapsed: 1:30:38.\n",
            "  Batch 16,450  of  29,322.    Elapsed: 1:30:54.\n",
            "  Batch 16,500  of  29,322.    Elapsed: 1:31:11.\n",
            "  Batch 16,550  of  29,322.    Elapsed: 1:31:28.\n",
            "  Batch 16,600  of  29,322.    Elapsed: 1:31:44.\n",
            "  Batch 16,650  of  29,322.    Elapsed: 1:32:01.\n",
            "  Batch 16,700  of  29,322.    Elapsed: 1:32:17.\n",
            "  Batch 16,750  of  29,322.    Elapsed: 1:32:34.\n",
            "  Batch 16,800  of  29,322.    Elapsed: 1:32:50.\n",
            "  Batch 16,850  of  29,322.    Elapsed: 1:33:07.\n",
            "  Batch 16,900  of  29,322.    Elapsed: 1:33:24.\n",
            "  Batch 16,950  of  29,322.    Elapsed: 1:33:40.\n",
            "  Batch 17,000  of  29,322.    Elapsed: 1:33:57.\n",
            "  Batch 17,050  of  29,322.    Elapsed: 1:34:13.\n",
            "  Batch 17,100  of  29,322.    Elapsed: 1:34:30.\n",
            "  Batch 17,150  of  29,322.    Elapsed: 1:34:46.\n",
            "  Batch 17,200  of  29,322.    Elapsed: 1:35:03.\n",
            "  Batch 17,250  of  29,322.    Elapsed: 1:35:20.\n",
            "  Batch 17,300  of  29,322.    Elapsed: 1:35:36.\n",
            "  Batch 17,350  of  29,322.    Elapsed: 1:35:53.\n",
            "  Batch 17,400  of  29,322.    Elapsed: 1:36:09.\n",
            "  Batch 17,450  of  29,322.    Elapsed: 1:36:26.\n",
            "  Batch 17,500  of  29,322.    Elapsed: 1:36:42.\n",
            "  Batch 17,550  of  29,322.    Elapsed: 1:36:59.\n",
            "  Batch 17,600  of  29,322.    Elapsed: 1:37:16.\n",
            "  Batch 17,650  of  29,322.    Elapsed: 1:37:32.\n",
            "  Batch 17,700  of  29,322.    Elapsed: 1:37:49.\n",
            "  Batch 17,750  of  29,322.    Elapsed: 1:38:05.\n",
            "  Batch 17,800  of  29,322.    Elapsed: 1:38:22.\n",
            "  Batch 17,850  of  29,322.    Elapsed: 1:38:38.\n",
            "  Batch 17,900  of  29,322.    Elapsed: 1:38:55.\n",
            "  Batch 17,950  of  29,322.    Elapsed: 1:39:12.\n",
            "  Batch 18,000  of  29,322.    Elapsed: 1:39:28.\n",
            "  Batch 18,050  of  29,322.    Elapsed: 1:39:45.\n",
            "  Batch 18,100  of  29,322.    Elapsed: 1:40:01.\n",
            "  Batch 18,150  of  29,322.    Elapsed: 1:40:18.\n",
            "  Batch 18,200  of  29,322.    Elapsed: 1:40:34.\n",
            "  Batch 18,250  of  29,322.    Elapsed: 1:40:51.\n",
            "  Batch 18,300  of  29,322.    Elapsed: 1:41:07.\n",
            "  Batch 18,350  of  29,322.    Elapsed: 1:41:24.\n",
            "  Batch 18,400  of  29,322.    Elapsed: 1:41:41.\n",
            "  Batch 18,450  of  29,322.    Elapsed: 1:41:57.\n",
            "  Batch 18,500  of  29,322.    Elapsed: 1:42:14.\n",
            "  Batch 18,550  of  29,322.    Elapsed: 1:42:30.\n",
            "  Batch 18,600  of  29,322.    Elapsed: 1:42:47.\n",
            "  Batch 18,650  of  29,322.    Elapsed: 1:43:04.\n",
            "  Batch 18,700  of  29,322.    Elapsed: 1:43:20.\n",
            "  Batch 18,750  of  29,322.    Elapsed: 1:43:37.\n",
            "  Batch 18,800  of  29,322.    Elapsed: 1:43:53.\n",
            "  Batch 18,850  of  29,322.    Elapsed: 1:44:10.\n",
            "  Batch 18,900  of  29,322.    Elapsed: 1:44:26.\n",
            "  Batch 18,950  of  29,322.    Elapsed: 1:44:43.\n",
            "  Batch 19,000  of  29,322.    Elapsed: 1:45:00.\n",
            "  Batch 19,050  of  29,322.    Elapsed: 1:45:16.\n",
            "  Batch 19,100  of  29,322.    Elapsed: 1:45:33.\n",
            "  Batch 19,150  of  29,322.    Elapsed: 1:45:49.\n",
            "  Batch 19,200  of  29,322.    Elapsed: 1:46:06.\n",
            "  Batch 19,250  of  29,322.    Elapsed: 1:46:22.\n",
            "  Batch 19,300  of  29,322.    Elapsed: 1:46:39.\n",
            "  Batch 19,350  of  29,322.    Elapsed: 1:46:55.\n",
            "  Batch 19,400  of  29,322.    Elapsed: 1:47:12.\n",
            "  Batch 19,450  of  29,322.    Elapsed: 1:47:29.\n",
            "  Batch 19,500  of  29,322.    Elapsed: 1:47:45.\n",
            "  Batch 19,550  of  29,322.    Elapsed: 1:48:02.\n",
            "  Batch 19,600  of  29,322.    Elapsed: 1:48:18.\n",
            "  Batch 19,650  of  29,322.    Elapsed: 1:48:35.\n",
            "  Batch 19,700  of  29,322.    Elapsed: 1:48:51.\n",
            "  Batch 19,750  of  29,322.    Elapsed: 1:49:08.\n",
            "  Batch 19,800  of  29,322.    Elapsed: 1:49:24.\n",
            "  Batch 19,850  of  29,322.    Elapsed: 1:49:41.\n",
            "  Batch 19,900  of  29,322.    Elapsed: 1:49:58.\n",
            "  Batch 19,950  of  29,322.    Elapsed: 1:50:14.\n",
            "  Batch 20,000  of  29,322.    Elapsed: 1:50:31.\n",
            "  Batch 20,050  of  29,322.    Elapsed: 1:50:47.\n",
            "  Batch 20,100  of  29,322.    Elapsed: 1:51:04.\n",
            "  Batch 20,150  of  29,322.    Elapsed: 1:51:20.\n",
            "  Batch 20,200  of  29,322.    Elapsed: 1:51:37.\n",
            "  Batch 20,250  of  29,322.    Elapsed: 1:51:54.\n",
            "  Batch 20,300  of  29,322.    Elapsed: 1:52:10.\n",
            "  Batch 20,350  of  29,322.    Elapsed: 1:52:27.\n",
            "  Batch 20,400  of  29,322.    Elapsed: 1:52:43.\n",
            "  Batch 20,450  of  29,322.    Elapsed: 1:53:00.\n",
            "  Batch 20,500  of  29,322.    Elapsed: 1:53:16.\n",
            "  Batch 20,550  of  29,322.    Elapsed: 1:53:33.\n",
            "  Batch 20,600  of  29,322.    Elapsed: 1:53:49.\n",
            "  Batch 20,650  of  29,322.    Elapsed: 1:54:06.\n",
            "  Batch 20,700  of  29,322.    Elapsed: 1:54:23.\n",
            "  Batch 20,750  of  29,322.    Elapsed: 1:54:39.\n",
            "  Batch 20,800  of  29,322.    Elapsed: 1:54:56.\n",
            "  Batch 20,850  of  29,322.    Elapsed: 1:55:12.\n",
            "  Batch 20,900  of  29,322.    Elapsed: 1:55:29.\n",
            "  Batch 20,950  of  29,322.    Elapsed: 1:55:45.\n",
            "  Batch 21,000  of  29,322.    Elapsed: 1:56:02.\n",
            "  Batch 21,050  of  29,322.    Elapsed: 1:56:18.\n",
            "  Batch 21,100  of  29,322.    Elapsed: 1:56:35.\n",
            "  Batch 21,150  of  29,322.    Elapsed: 1:56:52.\n",
            "  Batch 21,200  of  29,322.    Elapsed: 1:57:08.\n",
            "  Batch 21,250  of  29,322.    Elapsed: 1:57:25.\n",
            "  Batch 21,300  of  29,322.    Elapsed: 1:57:41.\n",
            "  Batch 21,350  of  29,322.    Elapsed: 1:57:58.\n",
            "  Batch 21,400  of  29,322.    Elapsed: 1:58:14.\n",
            "  Batch 21,450  of  29,322.    Elapsed: 1:58:31.\n",
            "  Batch 21,500  of  29,322.    Elapsed: 1:58:47.\n",
            "  Batch 21,550  of  29,322.    Elapsed: 1:59:04.\n",
            "  Batch 21,600  of  29,322.    Elapsed: 1:59:21.\n",
            "  Batch 21,650  of  29,322.    Elapsed: 1:59:37.\n",
            "  Batch 21,700  of  29,322.    Elapsed: 1:59:54.\n",
            "  Batch 21,750  of  29,322.    Elapsed: 2:00:10.\n",
            "  Batch 21,800  of  29,322.    Elapsed: 2:00:27.\n",
            "  Batch 21,850  of  29,322.    Elapsed: 2:00:43.\n",
            "  Batch 21,900  of  29,322.    Elapsed: 2:01:00.\n",
            "  Batch 21,950  of  29,322.    Elapsed: 2:01:16.\n",
            "  Batch 22,000  of  29,322.    Elapsed: 2:01:33.\n",
            "  Batch 22,050  of  29,322.    Elapsed: 2:01:50.\n",
            "  Batch 22,100  of  29,322.    Elapsed: 2:02:06.\n",
            "  Batch 22,150  of  29,322.    Elapsed: 2:02:23.\n",
            "  Batch 22,200  of  29,322.    Elapsed: 2:02:39.\n",
            "  Batch 22,250  of  29,322.    Elapsed: 2:02:56.\n",
            "  Batch 22,300  of  29,322.    Elapsed: 2:03:12.\n",
            "  Batch 22,350  of  29,322.    Elapsed: 2:03:29.\n",
            "  Batch 22,400  of  29,322.    Elapsed: 2:03:46.\n",
            "  Batch 22,450  of  29,322.    Elapsed: 2:04:02.\n",
            "  Batch 22,500  of  29,322.    Elapsed: 2:04:19.\n",
            "  Batch 22,550  of  29,322.    Elapsed: 2:04:35.\n",
            "  Batch 22,600  of  29,322.    Elapsed: 2:04:52.\n",
            "  Batch 22,650  of  29,322.    Elapsed: 2:05:08.\n",
            "  Batch 22,700  of  29,322.    Elapsed: 2:05:25.\n",
            "  Batch 22,750  of  29,322.    Elapsed: 2:05:41.\n",
            "  Batch 22,800  of  29,322.    Elapsed: 2:05:58.\n",
            "  Batch 22,850  of  29,322.    Elapsed: 2:06:15.\n",
            "  Batch 22,900  of  29,322.    Elapsed: 2:06:31.\n",
            "  Batch 22,950  of  29,322.    Elapsed: 2:06:48.\n",
            "  Batch 23,000  of  29,322.    Elapsed: 2:07:04.\n",
            "  Batch 23,050  of  29,322.    Elapsed: 2:07:21.\n",
            "  Batch 23,100  of  29,322.    Elapsed: 2:07:37.\n",
            "  Batch 23,150  of  29,322.    Elapsed: 2:07:54.\n",
            "  Batch 23,200  of  29,322.    Elapsed: 2:08:10.\n",
            "  Batch 23,250  of  29,322.    Elapsed: 2:08:27.\n",
            "  Batch 23,300  of  29,322.    Elapsed: 2:08:44.\n",
            "  Batch 23,350  of  29,322.    Elapsed: 2:09:00.\n",
            "  Batch 23,400  of  29,322.    Elapsed: 2:09:17.\n",
            "  Batch 23,450  of  29,322.    Elapsed: 2:09:33.\n",
            "  Batch 23,500  of  29,322.    Elapsed: 2:09:50.\n",
            "  Batch 23,550  of  29,322.    Elapsed: 2:10:06.\n",
            "  Batch 23,600  of  29,322.    Elapsed: 2:10:23.\n",
            "  Batch 23,650  of  29,322.    Elapsed: 2:10:39.\n",
            "  Batch 23,700  of  29,322.    Elapsed: 2:10:56.\n",
            "  Batch 23,750  of  29,322.    Elapsed: 2:11:13.\n",
            "  Batch 23,800  of  29,322.    Elapsed: 2:11:29.\n",
            "  Batch 23,850  of  29,322.    Elapsed: 2:11:46.\n",
            "  Batch 23,900  of  29,322.    Elapsed: 2:12:02.\n",
            "  Batch 23,950  of  29,322.    Elapsed: 2:12:19.\n",
            "  Batch 24,000  of  29,322.    Elapsed: 2:12:35.\n",
            "  Batch 24,050  of  29,322.    Elapsed: 2:12:52.\n",
            "  Batch 24,100  of  29,322.    Elapsed: 2:13:08.\n",
            "  Batch 24,150  of  29,322.    Elapsed: 2:13:25.\n",
            "  Batch 24,200  of  29,322.    Elapsed: 2:13:42.\n",
            "  Batch 24,250  of  29,322.    Elapsed: 2:13:58.\n",
            "  Batch 24,300  of  29,322.    Elapsed: 2:14:15.\n",
            "  Batch 24,350  of  29,322.    Elapsed: 2:14:31.\n",
            "  Batch 24,400  of  29,322.    Elapsed: 2:14:48.\n",
            "  Batch 24,450  of  29,322.    Elapsed: 2:15:04.\n",
            "  Batch 24,500  of  29,322.    Elapsed: 2:15:21.\n",
            "  Batch 24,550  of  29,322.    Elapsed: 2:15:38.\n",
            "  Batch 24,600  of  29,322.    Elapsed: 2:15:54.\n",
            "  Batch 24,650  of  29,322.    Elapsed: 2:16:11.\n",
            "  Batch 24,700  of  29,322.    Elapsed: 2:16:27.\n",
            "  Batch 24,750  of  29,322.    Elapsed: 2:16:44.\n",
            "  Batch 24,800  of  29,322.    Elapsed: 2:17:00.\n",
            "  Batch 24,850  of  29,322.    Elapsed: 2:17:17.\n",
            "  Batch 24,900  of  29,322.    Elapsed: 2:17:34.\n",
            "  Batch 24,950  of  29,322.    Elapsed: 2:17:50.\n",
            "  Batch 25,000  of  29,322.    Elapsed: 2:18:07.\n",
            "  Batch 25,050  of  29,322.    Elapsed: 2:18:23.\n",
            "  Batch 25,100  of  29,322.    Elapsed: 2:18:40.\n",
            "  Batch 25,150  of  29,322.    Elapsed: 2:18:56.\n",
            "  Batch 25,200  of  29,322.    Elapsed: 2:19:13.\n",
            "  Batch 25,250  of  29,322.    Elapsed: 2:19:29.\n",
            "  Batch 25,300  of  29,322.    Elapsed: 2:19:46.\n",
            "  Batch 25,350  of  29,322.    Elapsed: 2:20:03.\n",
            "  Batch 25,400  of  29,322.    Elapsed: 2:20:19.\n",
            "  Batch 25,450  of  29,322.    Elapsed: 2:20:36.\n",
            "  Batch 25,500  of  29,322.    Elapsed: 2:20:52.\n",
            "  Batch 25,550  of  29,322.    Elapsed: 2:21:09.\n",
            "  Batch 25,600  of  29,322.    Elapsed: 2:21:25.\n",
            "  Batch 25,650  of  29,322.    Elapsed: 2:21:42.\n",
            "  Batch 25,700  of  29,322.    Elapsed: 2:21:59.\n",
            "  Batch 25,750  of  29,322.    Elapsed: 2:22:15.\n",
            "  Batch 25,800  of  29,322.    Elapsed: 2:22:32.\n",
            "  Batch 25,850  of  29,322.    Elapsed: 2:22:48.\n",
            "  Batch 25,900  of  29,322.    Elapsed: 2:23:05.\n",
            "  Batch 25,950  of  29,322.    Elapsed: 2:23:21.\n",
            "  Batch 26,000  of  29,322.    Elapsed: 2:23:38.\n",
            "  Batch 26,050  of  29,322.    Elapsed: 2:23:55.\n",
            "  Batch 26,100  of  29,322.    Elapsed: 2:24:11.\n",
            "  Batch 26,150  of  29,322.    Elapsed: 2:24:28.\n",
            "  Batch 26,200  of  29,322.    Elapsed: 2:24:44.\n",
            "  Batch 26,250  of  29,322.    Elapsed: 2:25:01.\n",
            "  Batch 26,300  of  29,322.    Elapsed: 2:25:17.\n",
            "  Batch 26,350  of  29,322.    Elapsed: 2:25:34.\n",
            "  Batch 26,400  of  29,322.    Elapsed: 2:25:51.\n",
            "  Batch 26,450  of  29,322.    Elapsed: 2:26:07.\n",
            "  Batch 26,500  of  29,322.    Elapsed: 2:26:24.\n",
            "  Batch 26,550  of  29,322.    Elapsed: 2:26:40.\n",
            "  Batch 26,600  of  29,322.    Elapsed: 2:26:57.\n",
            "  Batch 26,650  of  29,322.    Elapsed: 2:27:13.\n",
            "  Batch 26,700  of  29,322.    Elapsed: 2:27:30.\n",
            "  Batch 26,750  of  29,322.    Elapsed: 2:27:46.\n",
            "  Batch 26,800  of  29,322.    Elapsed: 2:28:03.\n",
            "  Batch 26,850  of  29,322.    Elapsed: 2:28:20.\n",
            "  Batch 26,900  of  29,322.    Elapsed: 2:28:36.\n",
            "  Batch 26,950  of  29,322.    Elapsed: 2:28:53.\n",
            "  Batch 27,000  of  29,322.    Elapsed: 2:29:09.\n",
            "  Batch 27,050  of  29,322.    Elapsed: 2:29:26.\n",
            "  Batch 27,100  of  29,322.    Elapsed: 2:29:42.\n",
            "  Batch 27,150  of  29,322.    Elapsed: 2:29:59.\n",
            "  Batch 27,200  of  29,322.    Elapsed: 2:30:15.\n",
            "  Batch 27,250  of  29,322.    Elapsed: 2:30:32.\n",
            "  Batch 27,300  of  29,322.    Elapsed: 2:30:49.\n",
            "  Batch 27,350  of  29,322.    Elapsed: 2:31:05.\n",
            "  Batch 27,400  of  29,322.    Elapsed: 2:31:22.\n",
            "  Batch 27,450  of  29,322.    Elapsed: 2:31:38.\n",
            "  Batch 27,500  of  29,322.    Elapsed: 2:31:55.\n",
            "  Batch 27,550  of  29,322.    Elapsed: 2:32:11.\n",
            "  Batch 27,600  of  29,322.    Elapsed: 2:32:28.\n",
            "  Batch 27,650  of  29,322.    Elapsed: 2:32:45.\n",
            "  Batch 27,700  of  29,322.    Elapsed: 2:33:01.\n",
            "  Batch 27,750  of  29,322.    Elapsed: 2:33:18.\n",
            "  Batch 27,800  of  29,322.    Elapsed: 2:33:34.\n",
            "  Batch 27,850  of  29,322.    Elapsed: 2:33:51.\n",
            "  Batch 27,900  of  29,322.    Elapsed: 2:34:07.\n",
            "  Batch 27,950  of  29,322.    Elapsed: 2:34:24.\n",
            "  Batch 28,000  of  29,322.    Elapsed: 2:34:40.\n",
            "  Batch 28,050  of  29,322.    Elapsed: 2:34:57.\n",
            "  Batch 28,100  of  29,322.    Elapsed: 2:35:14.\n",
            "  Batch 28,150  of  29,322.    Elapsed: 2:35:30.\n",
            "  Batch 28,200  of  29,322.    Elapsed: 2:35:47.\n",
            "  Batch 28,250  of  29,322.    Elapsed: 2:36:03.\n",
            "  Batch 28,300  of  29,322.    Elapsed: 2:36:20.\n",
            "  Batch 28,350  of  29,322.    Elapsed: 2:36:36.\n",
            "  Batch 28,400  of  29,322.    Elapsed: 2:36:53.\n",
            "  Batch 28,450  of  29,322.    Elapsed: 2:37:10.\n",
            "  Batch 28,500  of  29,322.    Elapsed: 2:37:26.\n",
            "  Batch 28,550  of  29,322.    Elapsed: 2:37:43.\n",
            "  Batch 28,600  of  29,322.    Elapsed: 2:37:59.\n",
            "  Batch 28,650  of  29,322.    Elapsed: 2:38:16.\n",
            "  Batch 28,700  of  29,322.    Elapsed: 2:38:33.\n",
            "  Batch 28,750  of  29,322.    Elapsed: 2:38:49.\n",
            "  Batch 28,800  of  29,322.    Elapsed: 2:39:06.\n",
            "  Batch 28,850  of  29,322.    Elapsed: 2:39:22.\n",
            "  Batch 28,900  of  29,322.    Elapsed: 2:39:39.\n",
            "  Batch 28,950  of  29,322.    Elapsed: 2:39:55.\n",
            "  Batch 29,000  of  29,322.    Elapsed: 2:40:12.\n",
            "  Batch 29,050  of  29,322.    Elapsed: 2:40:29.\n",
            "  Batch 29,100  of  29,322.    Elapsed: 2:40:45.\n",
            "  Batch 29,150  of  29,322.    Elapsed: 2:41:02.\n",
            "  Batch 29,200  of  29,322.    Elapsed: 2:41:18.\n",
            "  Batch 29,250  of  29,322.    Elapsed: 2:41:35.\n",
            "  Batch 29,300  of  29,322.    Elapsed: 2:41:52.\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 2:41:59\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.30\n",
            "  Validation took: 0:05:39\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    50  of  29,322.    Elapsed: 0:00:17.\n",
            "  Batch   100  of  29,322.    Elapsed: 0:00:33.\n",
            "  Batch   150  of  29,322.    Elapsed: 0:00:50.\n",
            "  Batch   200  of  29,322.    Elapsed: 0:01:06.\n",
            "  Batch   250  of  29,322.    Elapsed: 0:01:23.\n",
            "  Batch   300  of  29,322.    Elapsed: 0:01:40.\n",
            "  Batch   350  of  29,322.    Elapsed: 0:01:56.\n",
            "  Batch   400  of  29,322.    Elapsed: 0:02:13.\n",
            "  Batch   450  of  29,322.    Elapsed: 0:02:29.\n",
            "  Batch   500  of  29,322.    Elapsed: 0:02:46.\n",
            "  Batch   550  of  29,322.    Elapsed: 0:03:02.\n",
            "  Batch   600  of  29,322.    Elapsed: 0:03:19.\n",
            "  Batch   650  of  29,322.    Elapsed: 0:03:36.\n",
            "  Batch   700  of  29,322.    Elapsed: 0:03:52.\n",
            "  Batch   750  of  29,322.    Elapsed: 0:04:09.\n",
            "  Batch   800  of  29,322.    Elapsed: 0:04:25.\n",
            "  Batch   850  of  29,322.    Elapsed: 0:04:42.\n",
            "  Batch   900  of  29,322.    Elapsed: 0:04:58.\n",
            "  Batch   950  of  29,322.    Elapsed: 0:05:15.\n",
            "  Batch 1,000  of  29,322.    Elapsed: 0:05:32.\n",
            "  Batch 1,050  of  29,322.    Elapsed: 0:05:48.\n",
            "  Batch 1,100  of  29,322.    Elapsed: 0:06:05.\n",
            "  Batch 1,150  of  29,322.    Elapsed: 0:06:21.\n",
            "  Batch 1,200  of  29,322.    Elapsed: 0:06:38.\n",
            "  Batch 1,250  of  29,322.    Elapsed: 0:06:55.\n",
            "  Batch 1,300  of  29,322.    Elapsed: 0:07:11.\n",
            "  Batch 1,350  of  29,322.    Elapsed: 0:07:28.\n",
            "  Batch 1,400  of  29,322.    Elapsed: 0:07:44.\n",
            "  Batch 1,450  of  29,322.    Elapsed: 0:08:01.\n",
            "  Batch 1,500  of  29,322.    Elapsed: 0:08:18.\n",
            "  Batch 1,550  of  29,322.    Elapsed: 0:08:34.\n",
            "  Batch 1,600  of  29,322.    Elapsed: 0:08:51.\n",
            "  Batch 1,650  of  29,322.    Elapsed: 0:09:07.\n",
            "  Batch 1,700  of  29,322.    Elapsed: 0:09:24.\n",
            "  Batch 1,750  of  29,322.    Elapsed: 0:09:41.\n",
            "  Batch 1,800  of  29,322.    Elapsed: 0:09:57.\n",
            "  Batch 1,850  of  29,322.    Elapsed: 0:10:14.\n",
            "  Batch 1,900  of  29,322.    Elapsed: 0:10:31.\n",
            "  Batch 1,950  of  29,322.    Elapsed: 0:10:47.\n",
            "  Batch 2,000  of  29,322.    Elapsed: 0:11:04.\n",
            "  Batch 2,050  of  29,322.    Elapsed: 0:11:20.\n",
            "  Batch 2,100  of  29,322.    Elapsed: 0:11:37.\n",
            "  Batch 2,150  of  29,322.    Elapsed: 0:11:54.\n",
            "  Batch 2,200  of  29,322.    Elapsed: 0:12:10.\n",
            "  Batch 2,250  of  29,322.    Elapsed: 0:12:27.\n",
            "  Batch 2,300  of  29,322.    Elapsed: 0:12:43.\n",
            "  Batch 2,350  of  29,322.    Elapsed: 0:13:00.\n",
            "  Batch 2,400  of  29,322.    Elapsed: 0:13:16.\n",
            "  Batch 2,450  of  29,322.    Elapsed: 0:13:33.\n",
            "  Batch 2,500  of  29,322.    Elapsed: 0:13:50.\n",
            "  Batch 2,550  of  29,322.    Elapsed: 0:14:06.\n",
            "  Batch 2,600  of  29,322.    Elapsed: 0:14:23.\n",
            "  Batch 2,650  of  29,322.    Elapsed: 0:14:39.\n",
            "  Batch 2,700  of  29,322.    Elapsed: 0:14:56.\n",
            "  Batch 2,750  of  29,322.    Elapsed: 0:15:12.\n",
            "  Batch 2,800  of  29,322.    Elapsed: 0:15:29.\n",
            "  Batch 2,850  of  29,322.    Elapsed: 0:15:46.\n",
            "  Batch 2,900  of  29,322.    Elapsed: 0:16:02.\n",
            "  Batch 2,950  of  29,322.    Elapsed: 0:16:19.\n",
            "  Batch 3,000  of  29,322.    Elapsed: 0:16:35.\n",
            "  Batch 3,050  of  29,322.    Elapsed: 0:16:52.\n",
            "  Batch 3,100  of  29,322.    Elapsed: 0:17:09.\n",
            "  Batch 3,150  of  29,322.    Elapsed: 0:17:25.\n",
            "  Batch 3,200  of  29,322.    Elapsed: 0:17:42.\n",
            "  Batch 3,250  of  29,322.    Elapsed: 0:17:58.\n",
            "  Batch 3,300  of  29,322.    Elapsed: 0:18:15.\n",
            "  Batch 3,350  of  29,322.    Elapsed: 0:18:31.\n",
            "  Batch 3,400  of  29,322.    Elapsed: 0:18:48.\n",
            "  Batch 3,450  of  29,322.    Elapsed: 0:19:05.\n",
            "  Batch 3,500  of  29,322.    Elapsed: 0:19:21.\n",
            "  Batch 3,550  of  29,322.    Elapsed: 0:19:38.\n",
            "  Batch 3,600  of  29,322.    Elapsed: 0:19:54.\n",
            "  Batch 3,650  of  29,322.    Elapsed: 0:20:11.\n",
            "  Batch 3,700  of  29,322.    Elapsed: 0:20:28.\n",
            "  Batch 3,750  of  29,322.    Elapsed: 0:20:44.\n",
            "  Batch 3,800  of  29,322.    Elapsed: 0:21:01.\n",
            "  Batch 3,850  of  29,322.    Elapsed: 0:21:17.\n",
            "  Batch 3,900  of  29,322.    Elapsed: 0:21:34.\n",
            "  Batch 3,950  of  29,322.    Elapsed: 0:21:50.\n",
            "  Batch 4,000  of  29,322.    Elapsed: 0:22:07.\n",
            "  Batch 4,050  of  29,322.    Elapsed: 0:22:24.\n",
            "  Batch 4,100  of  29,322.    Elapsed: 0:22:40.\n",
            "  Batch 4,150  of  29,322.    Elapsed: 0:22:57.\n",
            "  Batch 4,200  of  29,322.    Elapsed: 0:23:13.\n",
            "  Batch 4,250  of  29,322.    Elapsed: 0:23:30.\n",
            "  Batch 4,300  of  29,322.    Elapsed: 0:23:46.\n",
            "  Batch 4,350  of  29,322.    Elapsed: 0:24:03.\n",
            "  Batch 4,400  of  29,322.    Elapsed: 0:24:20.\n",
            "  Batch 4,450  of  29,322.    Elapsed: 0:24:36.\n",
            "  Batch 4,500  of  29,322.    Elapsed: 0:24:53.\n",
            "  Batch 4,550  of  29,322.    Elapsed: 0:25:09.\n",
            "  Batch 4,600  of  29,322.    Elapsed: 0:25:26.\n",
            "  Batch 4,650  of  29,322.    Elapsed: 0:25:42.\n",
            "  Batch 4,700  of  29,322.    Elapsed: 0:25:59.\n",
            "  Batch 4,750  of  29,322.    Elapsed: 0:26:16.\n",
            "  Batch 4,800  of  29,322.    Elapsed: 0:26:32.\n",
            "  Batch 4,850  of  29,322.    Elapsed: 0:26:49.\n",
            "  Batch 4,900  of  29,322.    Elapsed: 0:27:05.\n",
            "  Batch 4,950  of  29,322.    Elapsed: 0:27:22.\n",
            "  Batch 5,000  of  29,322.    Elapsed: 0:27:38.\n",
            "  Batch 5,050  of  29,322.    Elapsed: 0:27:55.\n",
            "  Batch 5,100  of  29,322.    Elapsed: 0:28:12.\n",
            "  Batch 5,150  of  29,322.    Elapsed: 0:28:28.\n",
            "  Batch 5,200  of  29,322.    Elapsed: 0:28:45.\n",
            "  Batch 5,250  of  29,322.    Elapsed: 0:29:01.\n",
            "  Batch 5,300  of  29,322.    Elapsed: 0:29:18.\n",
            "  Batch 5,350  of  29,322.    Elapsed: 0:29:34.\n",
            "  Batch 5,400  of  29,322.    Elapsed: 0:29:51.\n",
            "  Batch 5,450  of  29,322.    Elapsed: 0:30:08.\n",
            "  Batch 5,500  of  29,322.    Elapsed: 0:30:24.\n",
            "  Batch 5,550  of  29,322.    Elapsed: 0:30:41.\n",
            "  Batch 5,600  of  29,322.    Elapsed: 0:30:57.\n",
            "  Batch 5,650  of  29,322.    Elapsed: 0:31:14.\n",
            "  Batch 5,700  of  29,322.    Elapsed: 0:31:30.\n",
            "  Batch 5,750  of  29,322.    Elapsed: 0:31:47.\n",
            "  Batch 5,800  of  29,322.    Elapsed: 0:32:04.\n",
            "  Batch 5,850  of  29,322.    Elapsed: 0:32:20.\n",
            "  Batch 5,900  of  29,322.    Elapsed: 0:32:37.\n",
            "  Batch 5,950  of  29,322.    Elapsed: 0:32:53.\n",
            "  Batch 6,000  of  29,322.    Elapsed: 0:33:10.\n",
            "  Batch 6,050  of  29,322.    Elapsed: 0:33:26.\n",
            "  Batch 6,100  of  29,322.    Elapsed: 0:33:43.\n",
            "  Batch 6,150  of  29,322.    Elapsed: 0:33:59.\n",
            "  Batch 6,200  of  29,322.    Elapsed: 0:34:16.\n",
            "  Batch 6,250  of  29,322.    Elapsed: 0:34:33.\n",
            "  Batch 6,300  of  29,322.    Elapsed: 0:34:49.\n",
            "  Batch 6,350  of  29,322.    Elapsed: 0:35:06.\n",
            "  Batch 6,400  of  29,322.    Elapsed: 0:35:22.\n",
            "  Batch 6,450  of  29,322.    Elapsed: 0:35:39.\n",
            "  Batch 6,500  of  29,322.    Elapsed: 0:35:55.\n",
            "  Batch 6,550  of  29,322.    Elapsed: 0:36:12.\n",
            "  Batch 6,600  of  29,322.    Elapsed: 0:36:29.\n",
            "  Batch 6,650  of  29,322.    Elapsed: 0:36:45.\n",
            "  Batch 6,700  of  29,322.    Elapsed: 0:37:02.\n",
            "  Batch 6,750  of  29,322.    Elapsed: 0:37:18.\n",
            "  Batch 6,800  of  29,322.    Elapsed: 0:37:35.\n",
            "  Batch 6,850  of  29,322.    Elapsed: 0:37:51.\n",
            "  Batch 6,900  of  29,322.    Elapsed: 0:38:08.\n",
            "  Batch 6,950  of  29,322.    Elapsed: 0:38:25.\n",
            "  Batch 7,000  of  29,322.    Elapsed: 0:38:41.\n",
            "  Batch 7,050  of  29,322.    Elapsed: 0:38:58.\n",
            "  Batch 7,100  of  29,322.    Elapsed: 0:39:14.\n",
            "  Batch 7,150  of  29,322.    Elapsed: 0:39:31.\n",
            "  Batch 7,200  of  29,322.    Elapsed: 0:39:47.\n",
            "  Batch 7,250  of  29,322.    Elapsed: 0:40:04.\n",
            "  Batch 7,300  of  29,322.    Elapsed: 0:40:20.\n",
            "  Batch 7,350  of  29,322.    Elapsed: 0:40:37.\n",
            "  Batch 7,400  of  29,322.    Elapsed: 0:40:54.\n",
            "  Batch 7,450  of  29,322.    Elapsed: 0:41:10.\n",
            "  Batch 7,500  of  29,322.    Elapsed: 0:41:27.\n",
            "  Batch 7,550  of  29,322.    Elapsed: 0:41:43.\n",
            "  Batch 7,600  of  29,322.    Elapsed: 0:42:00.\n",
            "  Batch 7,650  of  29,322.    Elapsed: 0:42:17.\n",
            "  Batch 7,700  of  29,322.    Elapsed: 0:42:33.\n",
            "  Batch 7,750  of  29,322.    Elapsed: 0:42:50.\n",
            "  Batch 7,800  of  29,322.    Elapsed: 0:43:06.\n",
            "  Batch 7,850  of  29,322.    Elapsed: 0:43:23.\n",
            "  Batch 7,900  of  29,322.    Elapsed: 0:43:39.\n",
            "  Batch 7,950  of  29,322.    Elapsed: 0:43:56.\n",
            "  Batch 8,000  of  29,322.    Elapsed: 0:44:13.\n",
            "  Batch 8,050  of  29,322.    Elapsed: 0:44:29.\n",
            "  Batch 8,100  of  29,322.    Elapsed: 0:44:46.\n",
            "  Batch 8,150  of  29,322.    Elapsed: 0:45:02.\n",
            "  Batch 8,200  of  29,322.    Elapsed: 0:45:19.\n",
            "  Batch 8,250  of  29,322.    Elapsed: 0:45:36.\n",
            "  Batch 8,300  of  29,322.    Elapsed: 0:45:52.\n",
            "  Batch 8,350  of  29,322.    Elapsed: 0:46:09.\n",
            "  Batch 8,400  of  29,322.    Elapsed: 0:46:25.\n",
            "  Batch 8,450  of  29,322.    Elapsed: 0:46:42.\n",
            "  Batch 8,500  of  29,322.    Elapsed: 0:46:58.\n",
            "  Batch 8,550  of  29,322.    Elapsed: 0:47:15.\n",
            "  Batch 8,600  of  29,322.    Elapsed: 0:47:32.\n",
            "  Batch 8,650  of  29,322.    Elapsed: 0:47:48.\n",
            "  Batch 8,700  of  29,322.    Elapsed: 0:48:05.\n",
            "  Batch 8,750  of  29,322.    Elapsed: 0:48:21.\n",
            "  Batch 8,800  of  29,322.    Elapsed: 0:48:38.\n",
            "  Batch 8,850  of  29,322.    Elapsed: 0:48:54.\n",
            "  Batch 8,900  of  29,322.    Elapsed: 0:49:11.\n",
            "  Batch 8,950  of  29,322.    Elapsed: 0:49:28.\n",
            "  Batch 9,000  of  29,322.    Elapsed: 0:49:44.\n",
            "  Batch 9,050  of  29,322.    Elapsed: 0:50:01.\n",
            "  Batch 9,100  of  29,322.    Elapsed: 0:50:17.\n",
            "  Batch 9,150  of  29,322.    Elapsed: 0:50:34.\n",
            "  Batch 9,200  of  29,322.    Elapsed: 0:50:50.\n",
            "  Batch 9,250  of  29,322.    Elapsed: 0:51:07.\n",
            "  Batch 9,300  of  29,322.    Elapsed: 0:51:24.\n",
            "  Batch 9,350  of  29,322.    Elapsed: 0:51:40.\n",
            "  Batch 9,400  of  29,322.    Elapsed: 0:51:57.\n",
            "  Batch 9,450  of  29,322.    Elapsed: 0:52:13.\n",
            "  Batch 9,500  of  29,322.    Elapsed: 0:52:30.\n",
            "  Batch 9,550  of  29,322.    Elapsed: 0:52:46.\n",
            "  Batch 9,600  of  29,322.    Elapsed: 0:53:03.\n",
            "  Batch 9,650  of  29,322.    Elapsed: 0:53:20.\n",
            "  Batch 9,700  of  29,322.    Elapsed: 0:53:36.\n",
            "  Batch 9,750  of  29,322.    Elapsed: 0:53:53.\n",
            "  Batch 9,800  of  29,322.    Elapsed: 0:54:09.\n",
            "  Batch 9,850  of  29,322.    Elapsed: 0:54:26.\n",
            "  Batch 9,900  of  29,322.    Elapsed: 0:54:43.\n",
            "  Batch 9,950  of  29,322.    Elapsed: 0:54:59.\n",
            "  Batch 10,000  of  29,322.    Elapsed: 0:55:16.\n",
            "  Batch 10,050  of  29,322.    Elapsed: 0:55:32.\n",
            "  Batch 10,100  of  29,322.    Elapsed: 0:55:49.\n",
            "  Batch 10,150  of  29,322.    Elapsed: 0:56:06.\n",
            "  Batch 10,200  of  29,322.    Elapsed: 0:56:22.\n",
            "  Batch 10,250  of  29,322.    Elapsed: 0:56:39.\n",
            "  Batch 10,300  of  29,322.    Elapsed: 0:56:55.\n",
            "  Batch 10,350  of  29,322.    Elapsed: 0:57:12.\n",
            "  Batch 10,400  of  29,322.    Elapsed: 0:57:28.\n",
            "  Batch 10,450  of  29,322.    Elapsed: 0:57:45.\n",
            "  Batch 10,500  of  29,322.    Elapsed: 0:58:02.\n",
            "  Batch 10,550  of  29,322.    Elapsed: 0:58:18.\n",
            "  Batch 10,600  of  29,322.    Elapsed: 0:58:35.\n",
            "  Batch 10,650  of  29,322.    Elapsed: 0:58:51.\n",
            "  Batch 10,700  of  29,322.    Elapsed: 0:59:08.\n",
            "  Batch 10,750  of  29,322.    Elapsed: 0:59:25.\n",
            "  Batch 10,800  of  29,322.    Elapsed: 0:59:41.\n",
            "  Batch 10,850  of  29,322.    Elapsed: 0:59:58.\n",
            "  Batch 10,900  of  29,322.    Elapsed: 1:00:14.\n",
            "  Batch 10,950  of  29,322.    Elapsed: 1:00:31.\n",
            "  Batch 11,000  of  29,322.    Elapsed: 1:00:48.\n",
            "  Batch 11,050  of  29,322.    Elapsed: 1:01:04.\n",
            "  Batch 11,100  of  29,322.    Elapsed: 1:01:21.\n",
            "  Batch 11,150  of  29,322.    Elapsed: 1:01:37.\n",
            "  Batch 11,200  of  29,322.    Elapsed: 1:01:54.\n",
            "  Batch 11,250  of  29,322.    Elapsed: 1:02:10.\n",
            "  Batch 11,300  of  29,322.    Elapsed: 1:02:27.\n",
            "  Batch 11,350  of  29,322.    Elapsed: 1:02:44.\n",
            "  Batch 11,400  of  29,322.    Elapsed: 1:03:00.\n",
            "  Batch 11,450  of  29,322.    Elapsed: 1:03:17.\n",
            "  Batch 11,500  of  29,322.    Elapsed: 1:03:33.\n",
            "  Batch 11,550  of  29,322.    Elapsed: 1:03:50.\n",
            "  Batch 11,600  of  29,322.    Elapsed: 1:04:06.\n",
            "  Batch 11,650  of  29,322.    Elapsed: 1:04:23.\n",
            "  Batch 11,700  of  29,322.    Elapsed: 1:04:40.\n",
            "  Batch 11,750  of  29,322.    Elapsed: 1:04:56.\n",
            "  Batch 11,800  of  29,322.    Elapsed: 1:05:13.\n",
            "  Batch 11,850  of  29,322.    Elapsed: 1:05:29.\n",
            "  Batch 11,900  of  29,322.    Elapsed: 1:05:46.\n",
            "  Batch 11,950  of  29,322.    Elapsed: 1:06:03.\n",
            "  Batch 12,000  of  29,322.    Elapsed: 1:06:19.\n",
            "  Batch 12,050  of  29,322.    Elapsed: 1:06:36.\n",
            "  Batch 12,100  of  29,322.    Elapsed: 1:06:52.\n",
            "  Batch 12,150  of  29,322.    Elapsed: 1:07:09.\n",
            "  Batch 12,200  of  29,322.    Elapsed: 1:07:26.\n",
            "  Batch 12,250  of  29,322.    Elapsed: 1:07:42.\n",
            "  Batch 12,300  of  29,322.    Elapsed: 1:07:59.\n",
            "  Batch 12,350  of  29,322.    Elapsed: 1:08:15.\n",
            "  Batch 12,400  of  29,322.    Elapsed: 1:08:32.\n",
            "  Batch 12,450  of  29,322.    Elapsed: 1:08:48.\n",
            "  Batch 12,500  of  29,322.    Elapsed: 1:09:05.\n",
            "  Batch 12,550  of  29,322.    Elapsed: 1:09:22.\n",
            "  Batch 12,600  of  29,322.    Elapsed: 1:09:38.\n",
            "  Batch 12,650  of  29,322.    Elapsed: 1:09:55.\n",
            "  Batch 12,700  of  29,322.    Elapsed: 1:10:11.\n",
            "  Batch 12,750  of  29,322.    Elapsed: 1:10:28.\n",
            "  Batch 12,800  of  29,322.    Elapsed: 1:10:44.\n",
            "  Batch 12,850  of  29,322.    Elapsed: 1:11:01.\n",
            "  Batch 12,900  of  29,322.    Elapsed: 1:11:18.\n",
            "  Batch 12,950  of  29,322.    Elapsed: 1:11:34.\n",
            "  Batch 13,000  of  29,322.    Elapsed: 1:11:51.\n",
            "  Batch 13,050  of  29,322.    Elapsed: 1:12:07.\n",
            "  Batch 13,100  of  29,322.    Elapsed: 1:12:24.\n",
            "  Batch 13,150  of  29,322.    Elapsed: 1:12:40.\n",
            "  Batch 13,200  of  29,322.    Elapsed: 1:12:57.\n",
            "  Batch 13,250  of  29,322.    Elapsed: 1:13:14.\n",
            "  Batch 13,300  of  29,322.    Elapsed: 1:13:30.\n",
            "  Batch 13,350  of  29,322.    Elapsed: 1:13:47.\n",
            "  Batch 13,400  of  29,322.    Elapsed: 1:14:03.\n",
            "  Batch 13,450  of  29,322.    Elapsed: 1:14:20.\n",
            "  Batch 13,500  of  29,322.    Elapsed: 1:14:36.\n",
            "  Batch 13,550  of  29,322.    Elapsed: 1:14:53.\n",
            "  Batch 13,600  of  29,322.    Elapsed: 1:15:10.\n",
            "  Batch 13,650  of  29,322.    Elapsed: 1:15:26.\n",
            "  Batch 13,700  of  29,322.    Elapsed: 1:15:43.\n",
            "  Batch 13,750  of  29,322.    Elapsed: 1:15:59.\n",
            "  Batch 13,800  of  29,322.    Elapsed: 1:16:16.\n",
            "  Batch 13,850  of  29,322.    Elapsed: 1:16:32.\n",
            "  Batch 13,900  of  29,322.    Elapsed: 1:16:49.\n",
            "  Batch 13,950  of  29,322.    Elapsed: 1:17:06.\n",
            "  Batch 14,000  of  29,322.    Elapsed: 1:17:22.\n",
            "  Batch 14,050  of  29,322.    Elapsed: 1:17:39.\n",
            "  Batch 14,100  of  29,322.    Elapsed: 1:17:55.\n",
            "  Batch 14,150  of  29,322.    Elapsed: 1:18:12.\n",
            "  Batch 14,200  of  29,322.    Elapsed: 1:18:28.\n",
            "  Batch 14,250  of  29,322.    Elapsed: 1:18:45.\n",
            "  Batch 14,300  of  29,322.    Elapsed: 1:19:02.\n",
            "  Batch 14,350  of  29,322.    Elapsed: 1:19:18.\n",
            "  Batch 14,400  of  29,322.    Elapsed: 1:19:35.\n",
            "  Batch 14,450  of  29,322.    Elapsed: 1:19:51.\n",
            "  Batch 14,500  of  29,322.    Elapsed: 1:20:08.\n",
            "  Batch 14,550  of  29,322.    Elapsed: 1:20:24.\n",
            "  Batch 14,600  of  29,322.    Elapsed: 1:20:41.\n",
            "  Batch 14,650  of  29,322.    Elapsed: 1:20:58.\n",
            "  Batch 14,700  of  29,322.    Elapsed: 1:21:14.\n",
            "  Batch 14,750  of  29,322.    Elapsed: 1:21:31.\n",
            "  Batch 14,800  of  29,322.    Elapsed: 1:21:47.\n",
            "  Batch 14,850  of  29,322.    Elapsed: 1:22:04.\n",
            "  Batch 14,900  of  29,322.    Elapsed: 1:22:20.\n",
            "  Batch 14,950  of  29,322.    Elapsed: 1:22:37.\n",
            "  Batch 15,000  of  29,322.    Elapsed: 1:22:53.\n",
            "  Batch 15,050  of  29,322.    Elapsed: 1:23:10.\n",
            "  Batch 15,100  of  29,322.    Elapsed: 1:23:27.\n",
            "  Batch 15,150  of  29,322.    Elapsed: 1:23:43.\n",
            "  Batch 15,200  of  29,322.    Elapsed: 1:24:00.\n",
            "  Batch 15,250  of  29,322.    Elapsed: 1:24:16.\n",
            "  Batch 15,300  of  29,322.    Elapsed: 1:24:33.\n",
            "  Batch 15,350  of  29,322.    Elapsed: 1:24:49.\n",
            "  Batch 15,400  of  29,322.    Elapsed: 1:25:06.\n",
            "  Batch 15,450  of  29,322.    Elapsed: 1:25:22.\n",
            "  Batch 15,500  of  29,322.    Elapsed: 1:25:39.\n",
            "  Batch 15,550  of  29,322.    Elapsed: 1:25:56.\n",
            "  Batch 15,600  of  29,322.    Elapsed: 1:26:12.\n",
            "  Batch 15,650  of  29,322.    Elapsed: 1:26:29.\n",
            "  Batch 15,700  of  29,322.    Elapsed: 1:26:45.\n",
            "  Batch 15,750  of  29,322.    Elapsed: 1:27:02.\n",
            "  Batch 15,800  of  29,322.    Elapsed: 1:27:18.\n",
            "  Batch 15,850  of  29,322.    Elapsed: 1:27:35.\n",
            "  Batch 15,900  of  29,322.    Elapsed: 1:27:52.\n",
            "  Batch 15,950  of  29,322.    Elapsed: 1:28:08.\n",
            "  Batch 16,000  of  29,322.    Elapsed: 1:28:25.\n",
            "  Batch 16,050  of  29,322.    Elapsed: 1:28:41.\n",
            "  Batch 16,100  of  29,322.    Elapsed: 1:28:58.\n",
            "  Batch 16,150  of  29,322.    Elapsed: 1:29:14.\n",
            "  Batch 16,200  of  29,322.    Elapsed: 1:29:31.\n",
            "  Batch 16,250  of  29,322.    Elapsed: 1:29:47.\n",
            "  Batch 16,300  of  29,322.    Elapsed: 1:30:04.\n",
            "  Batch 16,350  of  29,322.    Elapsed: 1:30:21.\n",
            "  Batch 16,400  of  29,322.    Elapsed: 1:30:37.\n",
            "  Batch 16,450  of  29,322.    Elapsed: 1:30:54.\n",
            "  Batch 16,500  of  29,322.    Elapsed: 1:31:10.\n",
            "  Batch 16,550  of  29,322.    Elapsed: 1:31:27.\n",
            "  Batch 16,600  of  29,322.    Elapsed: 1:31:43.\n",
            "  Batch 16,650  of  29,322.    Elapsed: 1:32:00.\n",
            "  Batch 16,700  of  29,322.    Elapsed: 1:32:16.\n",
            "  Batch 16,750  of  29,322.    Elapsed: 1:32:33.\n",
            "  Batch 16,800  of  29,322.    Elapsed: 1:32:50.\n",
            "  Batch 16,850  of  29,322.    Elapsed: 1:33:06.\n",
            "  Batch 16,900  of  29,322.    Elapsed: 1:33:23.\n",
            "  Batch 16,950  of  29,322.    Elapsed: 1:33:39.\n",
            "  Batch 17,000  of  29,322.    Elapsed: 1:33:56.\n",
            "  Batch 17,050  of  29,322.    Elapsed: 1:34:12.\n",
            "  Batch 17,100  of  29,322.    Elapsed: 1:34:29.\n",
            "  Batch 17,150  of  29,322.    Elapsed: 1:34:45.\n",
            "  Batch 17,200  of  29,322.    Elapsed: 1:35:02.\n",
            "  Batch 17,250  of  29,322.    Elapsed: 1:35:19.\n",
            "  Batch 17,300  of  29,322.    Elapsed: 1:35:35.\n",
            "  Batch 17,350  of  29,322.    Elapsed: 1:35:52.\n",
            "  Batch 17,400  of  29,322.    Elapsed: 1:36:08.\n",
            "  Batch 17,450  of  29,322.    Elapsed: 1:36:25.\n",
            "  Batch 17,500  of  29,322.    Elapsed: 1:36:41.\n",
            "  Batch 17,550  of  29,322.    Elapsed: 1:36:58.\n",
            "  Batch 17,600  of  29,322.    Elapsed: 1:37:14.\n",
            "  Batch 17,650  of  29,322.    Elapsed: 1:37:31.\n",
            "  Batch 17,700  of  29,322.    Elapsed: 1:37:47.\n",
            "  Batch 17,750  of  29,322.    Elapsed: 1:38:04.\n",
            "  Batch 17,800  of  29,322.    Elapsed: 1:38:21.\n",
            "  Batch 17,850  of  29,322.    Elapsed: 1:38:37.\n",
            "  Batch 17,900  of  29,322.    Elapsed: 1:38:54.\n",
            "  Batch 17,950  of  29,322.    Elapsed: 1:39:10.\n",
            "  Batch 18,000  of  29,322.    Elapsed: 1:39:27.\n",
            "  Batch 18,050  of  29,322.    Elapsed: 1:39:43.\n",
            "  Batch 18,100  of  29,322.    Elapsed: 1:40:00.\n",
            "  Batch 18,150  of  29,322.    Elapsed: 1:40:17.\n",
            "  Batch 18,200  of  29,322.    Elapsed: 1:40:33.\n",
            "  Batch 18,250  of  29,322.    Elapsed: 1:40:50.\n",
            "  Batch 18,300  of  29,322.    Elapsed: 1:41:06.\n",
            "  Batch 18,350  of  29,322.    Elapsed: 1:41:23.\n",
            "  Batch 18,400  of  29,322.    Elapsed: 1:41:39.\n",
            "  Batch 18,450  of  29,322.    Elapsed: 1:41:56.\n",
            "  Batch 18,500  of  29,322.    Elapsed: 1:42:12.\n",
            "  Batch 18,550  of  29,322.    Elapsed: 1:42:29.\n",
            "  Batch 18,600  of  29,322.    Elapsed: 1:42:45.\n",
            "  Batch 18,650  of  29,322.    Elapsed: 1:43:02.\n",
            "  Batch 18,700  of  29,322.    Elapsed: 1:43:19.\n",
            "  Batch 18,750  of  29,322.    Elapsed: 1:43:35.\n",
            "  Batch 18,800  of  29,322.    Elapsed: 1:43:52.\n",
            "  Batch 18,850  of  29,322.    Elapsed: 1:44:08.\n",
            "  Batch 18,900  of  29,322.    Elapsed: 1:44:25.\n",
            "  Batch 18,950  of  29,322.    Elapsed: 1:44:41.\n",
            "  Batch 19,000  of  29,322.    Elapsed: 1:44:58.\n",
            "  Batch 19,050  of  29,322.    Elapsed: 1:45:14.\n",
            "  Batch 19,100  of  29,322.    Elapsed: 1:45:31.\n",
            "  Batch 19,150  of  29,322.    Elapsed: 1:45:48.\n",
            "  Batch 19,200  of  29,322.    Elapsed: 1:46:04.\n",
            "  Batch 19,250  of  29,322.    Elapsed: 1:46:21.\n",
            "  Batch 19,300  of  29,322.    Elapsed: 1:46:37.\n",
            "  Batch 19,350  of  29,322.    Elapsed: 1:46:54.\n",
            "  Batch 19,400  of  29,322.    Elapsed: 1:47:10.\n",
            "  Batch 19,450  of  29,322.    Elapsed: 1:47:27.\n",
            "  Batch 19,500  of  29,322.    Elapsed: 1:47:43.\n",
            "  Batch 19,550  of  29,322.    Elapsed: 1:48:00.\n",
            "  Batch 19,600  of  29,322.    Elapsed: 1:48:17.\n",
            "  Batch 19,650  of  29,322.    Elapsed: 1:48:33.\n",
            "  Batch 19,700  of  29,322.    Elapsed: 1:48:50.\n",
            "  Batch 19,750  of  29,322.    Elapsed: 1:49:06.\n",
            "  Batch 19,800  of  29,322.    Elapsed: 1:49:23.\n",
            "  Batch 19,850  of  29,322.    Elapsed: 1:49:39.\n",
            "  Batch 19,900  of  29,322.    Elapsed: 1:49:56.\n",
            "  Batch 19,950  of  29,322.    Elapsed: 1:50:12.\n",
            "  Batch 20,000  of  29,322.    Elapsed: 1:50:29.\n",
            "  Batch 20,050  of  29,322.    Elapsed: 1:50:46.\n",
            "  Batch 20,100  of  29,322.    Elapsed: 1:51:02.\n",
            "  Batch 20,150  of  29,322.    Elapsed: 1:51:19.\n",
            "  Batch 20,200  of  29,322.    Elapsed: 1:51:35.\n",
            "  Batch 20,250  of  29,322.    Elapsed: 1:51:52.\n",
            "  Batch 20,300  of  29,322.    Elapsed: 1:52:08.\n",
            "  Batch 20,350  of  29,322.    Elapsed: 1:52:25.\n",
            "  Batch 20,400  of  29,322.    Elapsed: 1:52:41.\n",
            "  Batch 20,450  of  29,322.    Elapsed: 1:52:58.\n",
            "  Batch 20,500  of  29,322.    Elapsed: 1:53:15.\n",
            "  Batch 20,550  of  29,322.    Elapsed: 1:53:31.\n",
            "  Batch 20,600  of  29,322.    Elapsed: 1:53:48.\n",
            "  Batch 20,650  of  29,322.    Elapsed: 1:54:04.\n",
            "  Batch 20,700  of  29,322.    Elapsed: 1:54:21.\n",
            "  Batch 20,750  of  29,322.    Elapsed: 1:54:37.\n",
            "  Batch 20,800  of  29,322.    Elapsed: 1:54:54.\n",
            "  Batch 20,850  of  29,322.    Elapsed: 1:55:10.\n",
            "  Batch 20,900  of  29,322.    Elapsed: 1:55:27.\n",
            "  Batch 20,950  of  29,322.    Elapsed: 1:55:44.\n",
            "  Batch 21,000  of  29,322.    Elapsed: 1:56:00.\n",
            "  Batch 21,050  of  29,322.    Elapsed: 1:56:17.\n",
            "  Batch 21,100  of  29,322.    Elapsed: 1:56:33.\n",
            "  Batch 21,150  of  29,322.    Elapsed: 1:56:50.\n",
            "  Batch 21,200  of  29,322.    Elapsed: 1:57:06.\n",
            "  Batch 21,250  of  29,322.    Elapsed: 1:57:23.\n",
            "  Batch 21,300  of  29,322.    Elapsed: 1:57:39.\n",
            "  Batch 21,350  of  29,322.    Elapsed: 1:57:56.\n",
            "  Batch 21,400  of  29,322.    Elapsed: 1:58:13.\n",
            "  Batch 21,450  of  29,322.    Elapsed: 1:58:29.\n",
            "  Batch 21,500  of  29,322.    Elapsed: 1:58:46.\n",
            "  Batch 21,550  of  29,322.    Elapsed: 1:59:02.\n",
            "  Batch 21,600  of  29,322.    Elapsed: 1:59:19.\n",
            "  Batch 21,650  of  29,322.    Elapsed: 1:59:35.\n",
            "  Batch 21,700  of  29,322.    Elapsed: 1:59:52.\n",
            "  Batch 21,750  of  29,322.    Elapsed: 2:00:09.\n",
            "  Batch 21,800  of  29,322.    Elapsed: 2:00:25.\n",
            "  Batch 21,850  of  29,322.    Elapsed: 2:00:42.\n",
            "  Batch 21,900  of  29,322.    Elapsed: 2:00:58.\n",
            "  Batch 21,950  of  29,322.    Elapsed: 2:01:15.\n",
            "  Batch 22,000  of  29,322.    Elapsed: 2:01:31.\n",
            "  Batch 22,050  of  29,322.    Elapsed: 2:01:48.\n",
            "  Batch 22,100  of  29,322.    Elapsed: 2:02:04.\n",
            "  Batch 22,150  of  29,322.    Elapsed: 2:02:21.\n",
            "  Batch 22,200  of  29,322.    Elapsed: 2:02:38.\n",
            "  Batch 22,250  of  29,322.    Elapsed: 2:02:54.\n",
            "  Batch 22,300  of  29,322.    Elapsed: 2:03:11.\n",
            "  Batch 22,350  of  29,322.    Elapsed: 2:03:27.\n",
            "  Batch 22,400  of  29,322.    Elapsed: 2:03:44.\n",
            "  Batch 22,450  of  29,322.    Elapsed: 2:04:00.\n",
            "  Batch 22,500  of  29,322.    Elapsed: 2:04:17.\n",
            "  Batch 22,550  of  29,322.    Elapsed: 2:04:33.\n",
            "  Batch 22,600  of  29,322.    Elapsed: 2:04:50.\n",
            "  Batch 22,650  of  29,322.    Elapsed: 2:05:07.\n",
            "  Batch 22,700  of  29,322.    Elapsed: 2:05:23.\n",
            "  Batch 22,750  of  29,322.    Elapsed: 2:05:40.\n",
            "  Batch 22,800  of  29,322.    Elapsed: 2:05:56.\n",
            "  Batch 22,850  of  29,322.    Elapsed: 2:06:13.\n",
            "  Batch 22,900  of  29,322.    Elapsed: 2:06:29.\n",
            "  Batch 22,950  of  29,322.    Elapsed: 2:06:46.\n",
            "  Batch 23,000  of  29,322.    Elapsed: 2:07:03.\n",
            "  Batch 23,050  of  29,322.    Elapsed: 2:07:19.\n",
            "  Batch 23,100  of  29,322.    Elapsed: 2:07:36.\n",
            "  Batch 23,150  of  29,322.    Elapsed: 2:07:52.\n",
            "  Batch 23,200  of  29,322.    Elapsed: 2:08:09.\n",
            "  Batch 23,250  of  29,322.    Elapsed: 2:08:25.\n",
            "  Batch 23,300  of  29,322.    Elapsed: 2:08:42.\n",
            "  Batch 23,350  of  29,322.    Elapsed: 2:08:58.\n",
            "  Batch 23,400  of  29,322.    Elapsed: 2:09:15.\n",
            "  Batch 23,450  of  29,322.    Elapsed: 2:09:32.\n",
            "  Batch 23,500  of  29,322.    Elapsed: 2:09:48.\n",
            "  Batch 23,550  of  29,322.    Elapsed: 2:10:05.\n",
            "  Batch 23,600  of  29,322.    Elapsed: 2:10:21.\n",
            "  Batch 23,650  of  29,322.    Elapsed: 2:10:38.\n",
            "  Batch 23,700  of  29,322.    Elapsed: 2:10:54.\n",
            "  Batch 23,750  of  29,322.    Elapsed: 2:11:11.\n",
            "  Batch 23,800  of  29,322.    Elapsed: 2:11:27.\n",
            "  Batch 23,850  of  29,322.    Elapsed: 2:11:44.\n",
            "  Batch 23,900  of  29,322.    Elapsed: 2:12:01.\n",
            "  Batch 23,950  of  29,322.    Elapsed: 2:12:17.\n",
            "  Batch 24,000  of  29,322.    Elapsed: 2:12:34.\n",
            "  Batch 24,050  of  29,322.    Elapsed: 2:12:50.\n",
            "  Batch 24,100  of  29,322.    Elapsed: 2:13:07.\n",
            "  Batch 24,150  of  29,322.    Elapsed: 2:13:23.\n",
            "  Batch 24,200  of  29,322.    Elapsed: 2:13:40.\n",
            "  Batch 24,250  of  29,322.    Elapsed: 2:13:57.\n",
            "  Batch 24,300  of  29,322.    Elapsed: 2:14:13.\n",
            "  Batch 24,350  of  29,322.    Elapsed: 2:14:30.\n",
            "  Batch 24,400  of  29,322.    Elapsed: 2:14:46.\n",
            "  Batch 24,450  of  29,322.    Elapsed: 2:15:03.\n",
            "  Batch 24,500  of  29,322.    Elapsed: 2:15:19.\n",
            "  Batch 24,550  of  29,322.    Elapsed: 2:15:36.\n",
            "  Batch 24,600  of  29,322.    Elapsed: 2:15:53.\n",
            "  Batch 24,650  of  29,322.    Elapsed: 2:16:09.\n",
            "  Batch 24,700  of  29,322.    Elapsed: 2:16:26.\n",
            "  Batch 24,750  of  29,322.    Elapsed: 2:16:42.\n",
            "  Batch 24,800  of  29,322.    Elapsed: 2:16:59.\n",
            "  Batch 24,850  of  29,322.    Elapsed: 2:17:16.\n",
            "  Batch 24,900  of  29,322.    Elapsed: 2:17:32.\n",
            "  Batch 24,950  of  29,322.    Elapsed: 2:17:49.\n",
            "  Batch 25,000  of  29,322.    Elapsed: 2:18:05.\n",
            "  Batch 25,050  of  29,322.    Elapsed: 2:18:22.\n",
            "  Batch 25,100  of  29,322.    Elapsed: 2:18:38.\n",
            "  Batch 25,150  of  29,322.    Elapsed: 2:18:55.\n",
            "  Batch 25,200  of  29,322.    Elapsed: 2:19:12.\n",
            "  Batch 25,250  of  29,322.    Elapsed: 2:19:28.\n",
            "  Batch 25,300  of  29,322.    Elapsed: 2:19:45.\n",
            "  Batch 25,350  of  29,322.    Elapsed: 2:20:01.\n",
            "  Batch 25,400  of  29,322.    Elapsed: 2:20:18.\n",
            "  Batch 25,450  of  29,322.    Elapsed: 2:20:34.\n",
            "  Batch 25,500  of  29,322.    Elapsed: 2:20:51.\n",
            "  Batch 25,550  of  29,322.    Elapsed: 2:21:08.\n",
            "  Batch 25,600  of  29,322.    Elapsed: 2:21:24.\n",
            "  Batch 25,650  of  29,322.    Elapsed: 2:21:41.\n",
            "  Batch 25,700  of  29,322.    Elapsed: 2:21:57.\n",
            "  Batch 25,750  of  29,322.    Elapsed: 2:22:14.\n",
            "  Batch 25,800  of  29,322.    Elapsed: 2:22:30.\n",
            "  Batch 25,850  of  29,322.    Elapsed: 2:22:47.\n",
            "  Batch 25,900  of  29,322.    Elapsed: 2:23:03.\n",
            "  Batch 25,950  of  29,322.    Elapsed: 2:23:20.\n",
            "  Batch 26,000  of  29,322.    Elapsed: 2:23:37.\n",
            "  Batch 26,050  of  29,322.    Elapsed: 2:23:53.\n",
            "  Batch 26,100  of  29,322.    Elapsed: 2:24:10.\n",
            "  Batch 26,150  of  29,322.    Elapsed: 2:24:26.\n",
            "  Batch 26,200  of  29,322.    Elapsed: 2:24:43.\n",
            "  Batch 26,250  of  29,322.    Elapsed: 2:24:59.\n",
            "  Batch 26,300  of  29,322.    Elapsed: 2:25:16.\n",
            "  Batch 26,350  of  29,322.    Elapsed: 2:25:32.\n",
            "  Batch 26,400  of  29,322.    Elapsed: 2:25:49.\n",
            "  Batch 26,450  of  29,322.    Elapsed: 2:26:06.\n",
            "  Batch 26,500  of  29,322.    Elapsed: 2:26:22.\n",
            "  Batch 26,550  of  29,322.    Elapsed: 2:26:39.\n",
            "  Batch 26,600  of  29,322.    Elapsed: 2:26:55.\n",
            "  Batch 26,650  of  29,322.    Elapsed: 2:27:12.\n",
            "  Batch 26,700  of  29,322.    Elapsed: 2:27:28.\n",
            "  Batch 26,750  of  29,322.    Elapsed: 2:27:45.\n",
            "  Batch 26,800  of  29,322.    Elapsed: 2:28:02.\n",
            "  Batch 26,850  of  29,322.    Elapsed: 2:28:18.\n",
            "  Batch 26,900  of  29,322.    Elapsed: 2:28:35.\n",
            "  Batch 26,950  of  29,322.    Elapsed: 2:28:51.\n",
            "  Batch 27,000  of  29,322.    Elapsed: 2:29:08.\n",
            "  Batch 27,050  of  29,322.    Elapsed: 2:29:24.\n",
            "  Batch 27,100  of  29,322.    Elapsed: 2:29:41.\n",
            "  Batch 27,150  of  29,322.    Elapsed: 2:29:58.\n",
            "  Batch 27,200  of  29,322.    Elapsed: 2:30:14.\n",
            "  Batch 27,250  of  29,322.    Elapsed: 2:30:31.\n",
            "  Batch 27,300  of  29,322.    Elapsed: 2:30:47.\n",
            "  Batch 27,350  of  29,322.    Elapsed: 2:31:04.\n",
            "  Batch 27,400  of  29,322.    Elapsed: 2:31:20.\n",
            "  Batch 27,450  of  29,322.    Elapsed: 2:31:37.\n",
            "  Batch 27,500  of  29,322.    Elapsed: 2:31:54.\n",
            "  Batch 27,550  of  29,322.    Elapsed: 2:32:10.\n",
            "  Batch 27,600  of  29,322.    Elapsed: 2:32:27.\n",
            "  Batch 27,650  of  29,322.    Elapsed: 2:32:43.\n",
            "  Batch 27,700  of  29,322.    Elapsed: 2:33:00.\n",
            "  Batch 27,750  of  29,322.    Elapsed: 2:33:16.\n",
            "  Batch 27,800  of  29,322.    Elapsed: 2:33:33.\n",
            "  Batch 27,850  of  29,322.    Elapsed: 2:33:49.\n",
            "  Batch 27,900  of  29,322.    Elapsed: 2:34:06.\n",
            "  Batch 27,950  of  29,322.    Elapsed: 2:34:23.\n",
            "  Batch 28,000  of  29,322.    Elapsed: 2:34:39.\n",
            "  Batch 28,050  of  29,322.    Elapsed: 2:34:56.\n",
            "  Batch 28,100  of  29,322.    Elapsed: 2:35:12.\n",
            "  Batch 28,150  of  29,322.    Elapsed: 2:35:29.\n",
            "  Batch 28,200  of  29,322.    Elapsed: 2:35:45.\n",
            "  Batch 28,250  of  29,322.    Elapsed: 2:36:02.\n",
            "  Batch 28,300  of  29,322.    Elapsed: 2:36:19.\n",
            "  Batch 28,350  of  29,322.    Elapsed: 2:36:35.\n",
            "  Batch 28,400  of  29,322.    Elapsed: 2:36:52.\n",
            "  Batch 28,450  of  29,322.    Elapsed: 2:37:08.\n",
            "  Batch 28,500  of  29,322.    Elapsed: 2:37:25.\n",
            "  Batch 28,550  of  29,322.    Elapsed: 2:37:41.\n",
            "  Batch 28,600  of  29,322.    Elapsed: 2:37:58.\n",
            "  Batch 28,650  of  29,322.    Elapsed: 2:38:15.\n",
            "  Batch 28,700  of  29,322.    Elapsed: 2:38:31.\n",
            "  Batch 28,750  of  29,322.    Elapsed: 2:38:48.\n",
            "  Batch 28,800  of  29,322.    Elapsed: 2:39:04.\n",
            "  Batch 28,850  of  29,322.    Elapsed: 2:39:21.\n",
            "  Batch 28,900  of  29,322.    Elapsed: 2:39:37.\n",
            "  Batch 28,950  of  29,322.    Elapsed: 2:39:54.\n",
            "  Batch 29,000  of  29,322.    Elapsed: 2:40:10.\n",
            "  Batch 29,050  of  29,322.    Elapsed: 2:40:27.\n",
            "  Batch 29,100  of  29,322.    Elapsed: 2:40:44.\n",
            "  Batch 29,150  of  29,322.    Elapsed: 2:41:00.\n",
            "  Batch 29,200  of  29,322.    Elapsed: 2:41:17.\n",
            "  Batch 29,250  of  29,322.    Elapsed: 2:41:33.\n",
            "  Batch 29,300  of  29,322.    Elapsed: 2:41:50.\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 2:41:57\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.36\n",
            "  Validation took: 0:05:38\n",
            "\n",
            "Training complete!\n",
            "Total training took 5:35:13 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlQ3dP6JWUMy",
        "colab_type": "text"
      },
      "source": [
        "### Step 7: Visualize Training Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD4C4QqXR-_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "9d0f46cf-d533-4c27-eb33-d187b9fdd351"
      },
      "source": [
        "#Make a dataframe of results. \n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.29</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.83</td>\n",
              "      <td>2:41:59</td>\n",
              "      <td>0:05:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.83</td>\n",
              "      <td>2:41:57</td>\n",
              "      <td>0:05:38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.29         0.30           0.83       2:41:59         0:05:39\n",
              "2               0.30         0.36           0.83       2:41:57         0:05:38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU00jJPFWZ6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "4ef5bf90-2e26-40f6-f03c-91d1623eac2c"
      },
      "source": [
        "#Plot the results from the Dataframe\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj66rRTTXPB0",
        "colab_type": "text"
      },
      "source": [
        "### Step 8: Prep Dev Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7lRfm72X4VE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9a8582d8-0fa5-4f4f-c08c-d1d185c33179"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#import Dev set. \n",
        "dev_df = pd.read_csv('dev_AB4.csv')\n",
        "#the empty choice is converted to a NaN when I reload, so this will correct the issue.\n",
        "dev_df['a'].fillna(\"\", inplace=True)\n",
        "\n",
        "print('Number of dev sentences: {:,}\\n'.format(dev_df.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of dev sentences: 11,873\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNTzPDlhfMNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pull out the relevant columns.\n",
        "\n",
        "contexts = dev_df.context.values\n",
        "questions = dev_df.question.values\n",
        "choices = dev_df[['a','b']].values\n",
        "#now converted to an INT\n",
        "dev_df.correct_index = dev_df.correct_index.fillna(0)\n",
        "labels = dev_df.correct_index.astype(int).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTxXkT3PemPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "choices_features = []\n",
        "\n",
        "#---- THIS IS THE LOOP TO COMBINE THE QUESTIONS WITH THE CHOICES ----\n",
        "for i in range(len(questions)):\n",
        "    row = list(choices[i])\n",
        "    q_text = str(questions[i])+' '+str(contexts[i])\n",
        "    temp_list = []\n",
        "    for choice in row:\n",
        "      text = (str(choice))\n",
        "      temp_list.append(text)\n",
        "\n",
        "    encoded_dict = tokenizer(\n",
        "                        [q_text,q_text],\n",
        "                        temp_list,\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 384,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation = True)\n",
        "\n",
        "# #Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# # Convert the lists into tensors.\n",
        "#input_ids = torch.cat(input_ids, dim=0)\n",
        "input_ids = torch.stack(input_ids)\n",
        "attention_masks = torch.stack(attention_masks)\n",
        "labels = torch.tensor(labels).long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Ye7Y4cf1GW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a5e27c86-4662-4d86-fe91-96f912a8841c"
      },
      "source": [
        "#Check the shape to make sure it worked correctly. \n",
        "print(input_ids.size(0))\n",
        "print(attention_masks.size(0))\n",
        "print(labels.size(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11873\n",
            "11873\n",
            "11873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqVe8OfDemNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the batch size.  \n",
        "batch_size = 2  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNg5M2QNXXbU",
        "colab_type": "text"
      },
      "source": [
        "### Step 9: Evaluate Dev Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFbS9e0bX461",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cea994b3-6551-4cf8-d6a7-b4ee82585fd5"
      },
      "source": [
        "# Prediction on test set\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 11,873 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q40urDnUgYXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label_list(predictions):\n",
        "  full_label_list = []\n",
        "  for i in range(len(predictions)):\n",
        "    for j in range(len(predictions[i])):\n",
        "      full_label_list.append((np.argmax(predictions[i], axis=1).flatten())[j])\n",
        "  return full_label_list\n",
        "\n",
        "def get_pred_dict(full_labels, df):\n",
        "    pred_dict = {}\n",
        "    for i in range(len(full_labels)):\n",
        "        key = str(dev_df['id'][i])\n",
        "        best_guess = str(df.iloc[i, full_labels[i]])\n",
        "        pred_dict[key] = best_guess\n",
        "    return pred_dict \n",
        "\n",
        "#Output dict into a csv.\n",
        "def output_predictions(pred_dict, file_name):\n",
        "    with open(file_name, 'w', encoding = 'utf-8') as json_file:\n",
        "        json.dump(pred_dict, json_file, ensure_ascii=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iks-HKOlgYjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "full_labels = get_label_list(predictions)\n",
        "choice_df = dev_df[['a','b']]\n",
        "preds = get_pred_dict(full_labels, choice_df)\n",
        "output_predictions(preds, 'preds_AB.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0RC14KxpCzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save a copy in my drive.\n",
        "\n",
        "%cp -R /content/preds_AB.json /content/drive/My\\ Drive/model_save "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxTBJ_gxpNBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0581bd6d-784b-4381-f036-8c73ee852bf3"
      },
      "source": [
        "# Clone SQUAD repo for the evaluation file.\n",
        "# Move the eval file to my content folder \n",
        "\n",
        "!git clone https://github.com/white127/SQUAD-2.0-bidaf.git\n",
        "%mv /content/SQUAD-2.0-bidaf/evaluate-v2.0.py /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SQUAD-2.0-bidaf'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Total 125 (delta 0), reused 0 (delta 0), pack-reused 125\u001b[K\n",
            "Receiving objects: 100% (125/125), 709.51 KiB | 13.91 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhqVVl0qqZYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "01e458e4-c611-4bd2-f293-5ea31325d4ef"
      },
      "source": [
        "# Still download the Dev set.\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-22 00:34:10--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.110.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: â€˜dev-v2.0.jsonâ€™\n",
            "\n",
            "\rdev-v2.0.json         0%[                    ]       0  --.-KB/s               \rdev-v2.0.json       100%[===================>]   4.17M  21.6MB/s    in 0.2s    \n",
            "\n",
            "2020-07-22 00:34:10 (21.6 MB/s) - â€˜dev-v2.0.jsonâ€™ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1w2J0GtpNGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "92b84c54-735a-4b1b-9316-c9e7f7df419e"
      },
      "source": [
        "print(\"Results for AB, with 2 way Mutli Choice\")\n",
        "!python evaluate-v2.0.py dev-v2.0.json preds_AB.json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for AB, with 2 way Mutli Choice\n",
            "{\n",
            "  \"exact\": 63.766529099637836,\n",
            "  \"f1\": 67.42261166125509,\n",
            "  \"total\": 11873,\n",
            "  \"HasAns_exact\": 75.67476383265857,\n",
            "  \"HasAns_f1\": 82.9974136730904,\n",
            "  \"HasAns_total\": 5928,\n",
            "  \"NoAns_exact\": 51.892346509671995,\n",
            "  \"NoAns_f1\": 51.892346509671995,\n",
            "  \"NoAns_total\": 5945\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luE3KtO7Xb08",
        "colab_type": "text"
      },
      "source": [
        "### Step 10: Save Fine-Tuned Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISe-Pv-cW-LY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32833ce6-f75e-435b-87bc-5fe3f847eedb"
      },
      "source": [
        "##### MAKE SURE YOU MOVE A COPY TO YOUR BUCKET.\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/AB_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/AB_save/model_state_dict.pth')\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "#torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/AB_save/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}